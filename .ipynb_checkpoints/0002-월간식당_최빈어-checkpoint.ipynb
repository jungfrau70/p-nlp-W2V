{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "based-basketball",
   "metadata": {
    "papermill": {
     "duration": 0.016002,
     "end_time": "2021-03-23T04:59:55.407235",
     "exception": false,
     "start_time": "2021-03-23T04:59:55.391233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 월간식당 빅데이터 분석\n",
    "\n",
    "* 키워드별 단어 출현 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-grass",
   "metadata": {
    "papermill": {
     "duration": 4.386822,
     "end_time": "2021-03-23T04:59:59.811243",
     "exception": false,
     "start_time": "2021-03-23T04:59:55.424421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install konlpy xlrd openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-italian",
   "metadata": {
    "papermill": {
     "duration": 2.519479,
     "end_time": "2021-03-23T05:00:02.349617",
     "exception": false,
     "start_time": "2021-03-23T04:59:59.830138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "from pandas import read_excel\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from msba import posts as p\n",
    "from msba import stopwords as stopswords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-terrain",
   "metadata": {
    "papermill": {
     "duration": 0.026232,
     "end_time": "2021-03-23T05:00:02.397444",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.371212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 키워드/관심어 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-fantasy",
   "metadata": {
    "papermill": {
     "duration": 0.210209,
     "end_time": "2021-03-23T05:00:02.619467",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.409258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interested_words():\n",
    "    # 핵심단어 읽어 오기\n",
    "    my_sheet = '소비키워드'\n",
    "    keywords_filename = 'deskresearch_.xlsx'\n",
    "    df = read_excel(keywords_filename, sheet_name = my_sheet, header=1) # index_col='번호'\n",
    "    keywords = df['핵심단어']\n",
    "    subkeywords = df['대체어']\n",
    "    interested_words = df['키워드']\n",
    "    return keywords, subkeywords, interested_words\n",
    "\n",
    "keywords, subkeywords, interested_words = interested_words()\n",
    "\n",
    "for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "    subkeyword = subkeyword.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "    interested_word = subkeyword + \"|\" + interested_word.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "#     print(keyword, \" : \" , subkeyword, interested_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-hawaiian",
   "metadata": {
    "papermill": {
     "duration": 0.01801,
     "end_time": "2021-03-23T05:00:02.650113",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.632103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-seller",
   "metadata": {
    "papermill": {
     "duration": 0.039665,
     "end_time": "2021-03-23T05:00:02.705340",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.665675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word = [ '1인미디어' ]\n",
    "stopwords = stopswords.stopwords\n",
    "stopwords_kr = stopwords + [\n",
    "'하지만', '그리고', '그런데', '저는','제가','있습니다', '않습니다', 'ㅋㅋ', 'ㅋㅋㅋ', 'ㅎㅎ', 'ㅎㅎㅎ',\n",
    "'그럼', '이런', '저런', '합니다', '있어요', '참고로', '그러고', '아시죠', '하는',\n",
    "'있어서', '그냥', '같아요', '입니다', 'com', '아주', '않습니다',\n",
    "'같은', '해서', '있고', '않고', '없는', '있는데',\n",
    "'www', '먼저', '다시', '있도록', '등을','대한','있으니','미리','것이','모든', '00','없이',\n",
    "'정도','오늘', '근데', '같습니다','통해', '내가','나는', '나오는', 'http', '바랍니다',\n",
    "'있어', '위한','요즘','한번','밝혔다','계속', '됩니다','사실', '더욱', '하기',\n",
    "'아닌', '쉬게', '여러', '가능합니다', '보고', '분들이', '있다는', '라는', '등의',\n",
    "'수도', '되었습니다', '부탁드립니다', '있는데요', '되고', '있게', '이를', '통한', '것으로', '있으며', '가지', \n",
    "'경우', '000원','이미','가서','지금','by','그런', '보니','on', '했는데', '전에','좋을',\n",
    "'위해', '만원', '인분', '인당', '배가', '완전', '자주', '양이', '오늘은', '먹었어요', '처음', '되는', '좋아요',\n",
    "'먹은', '것은', '먹으니', '높은', '정도로', '넣고', 'ㅠㅠ', '나왔어요', 'ㅋㅋㅋㅋ', '아직',\n",
    "'가는', '것도', '것을','있다고', '기자', '따른','보면','알고','있을','11','10','19',\n",
    "'주는', '되면', '매우', '보다', '대신', '때문이다', '가진', '이는', '출처', '우리는', '앞으로', \n",
    "'했다', '곳이', '한다', '이렇게', '하고', '된다', '매우', '의한', '설명하시오', '아래와', '이후', '교시', '들어', '있지만',\n",
    "'것이다', '출처', '나의', '말합니다', '나를', '된다', '인해', '가능한', '관련', '하지', '없다', '비건', \n",
    "'image', '이제', '라고','있었어요','해도','좋습니다', '없고','위에','나온', '않는', '가지고','않은','그렇게','하는데',\n",
    "'그런지','많아서','것입니다','이건','또는', '있었습니다', '되어', '없습니다', '싶은',\n",
    "'이거', '사용할', '전혀', '아예', '표시되지', '살짝', '오는', '다들', '밖에', '요즘은', \n",
    "'개의', 'ㅋㅋㅋㅋㅋ', '안에', '이상의', '생각합니다', '넘어갑니다더싸다특가', '등으로', '가면', '파는',\n",
    "'하네요', '넣어', '하니', '하네요', '사용한', '개의', '년까지', '했습니다', '하게', '하니', '하네요', '있기', '있으면', \n",
    "'아니다', '같아서', '먹일', '것이라는', '보인다', '사상', '만명', \n",
    "'않는다', '있다면', '하는데요', '하니', '하네요', '일부터', '뭔가', '와의', '것이라', '아닐까', '주고',\n",
    "'것에', '것이라고', '그것을', '누구나', '두고', '싶습니다', '중에', '못하는', '것이죠', '한다는', '그렇다면', \n",
    "'봅니다', '수밖에', '싶네요', '맛있어요', '맛있었어요', '올려서', '오랜만에', '주문했어요', '우리가', '사람이',\n",
    "'그의', '헤헤헤', '기반', '그는', '이라는', '이다', '말했다', '그들의', '주로', '이번에', '아래', '만큼', '억원',\n",
    "'받고', '해야', '있죠', '받을', '오전', '오후', '나타났다', '반면', '뜻하는', '대상으로', '관계자는', \n",
    "'갖고', '다음과', '달러', '먹으면', '아니고', '있음', '예상', '경우가', '원하는', '뜻하는', '이제는', '년간', \n",
    "'와서', '이게', '있었다', '지난', '사용하는', '번째', '등이', '이하', '인한', '이에', '서비스입니다', '있는', '그대로', \n",
    "'있답니다', '일까지', '위치한', '대해', '번째', '먹고', '먹기', '먹는', '먹을', '마시고', '먹어도', '먹었는데', '먹어야', '먹었던',\n",
    "'마시는', '마시면', '가장', '엄청', '살짝', '같은데', '넘나', \n",
    "'확실히', '등은', '지난달', '대부분', \n",
    "'하면', 'photo','아니라','역시','저희는', '저도','특히','30', '따로', '안녕하세요','항상','현재',\n",
    "'많은', '많이', '정말', '너무', 'https', 'Coupang', 'Coupa', 'ng'\n",
    "] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-chile",
   "metadata": {
    "papermill": {
     "duration": 0.022677,
     "end_time": "2021-03-23T05:00:02.744443",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.721766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 자신의 관심사에 맞는 단어로 데이터 가져오기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "agreed-norman",
   "metadata": {
    "papermill": {
     "duration": 0.027008,
     "end_time": "2021-03-23T05:00:02.798585",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.771577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 관심 기간 : 2019.7.1 ~ 2019.6.30\n",
    "### 추가 필터 : 핵심단어 별 관심단어\n",
    "### 레코드 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-direction",
   "metadata": {
    "papermill": {
     "duration": 0.033551,
     "end_time": "2021-03-23T05:00:02.848433",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.814882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # 개행문자 제거\n",
    "    text = re.sub('\\\\\\\\n', ' ', text)\n",
    "    text = re.sub('[A-z]', '', text)\n",
    "    text = re.sub('[0-9]', '', text)\n",
    "    res = ''.join([i for i in text if not i.isdigit()]) \n",
    "    return text\n",
    "\n",
    "def preprocessing_2(rows):\n",
    "    pattern = re.compile(r\"[.,?!★~]\")\n",
    "    sentences = []\n",
    "    for row in rows:\n",
    "        sentences = sentences + pattern.split(row.replace(\"\\xa0\", \"\").replace(\"\\t\",\"\").strip())\n",
    "        sentences = list(set(sentences))        \n",
    "    return sentences\n",
    "\n",
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)\n",
    "\n",
    "# 실전 : 말뭉치 생성 및 전처리\n",
    "def preprocessing_3(sentences):\n",
    "    # Convert list of strings to string\n",
    "#     full_str = convert_list_to_string(sentences[1])\n",
    "#     # print(full_str)\n",
    "\n",
    "    pattern = re.compile(r\".*(광고정보).*\")\n",
    "\n",
    "    corpus = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence):\n",
    "            sentence = sentence.strip().split(\" \")\n",
    "            sentence = convert_list_to_string(sentence).replace(\"  \", \" \").strip()\n",
    "            if pattern.match(sentence):\n",
    "                continue\n",
    "            else:\n",
    "                corpus.append(sentence)\n",
    "    return corpus\n",
    "\n",
    "def preprocessing_4(docs):\n",
    "    # docs = [\n",
    "    #         w for w in hannanum.nouns(\" \".join(cell)) for cell in cells\n",
    "    #         if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))\n",
    "    # ]\n",
    "    vect = CountVectorizer(stop_words=stopwords_kr, min_df=5, max_df=200).fit(docs)\n",
    "    count = vect.transform(docs).toarray().sum(axis=0)\n",
    "    idx = np.argsort(-count)\n",
    "    count = count[idx]\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]\n",
    "    # plt.bar(range(len(count)), count)\n",
    "    # plt.show()\n",
    "\n",
    "    tf_list = list(zip(feature_name, count))[:100]\n",
    "\n",
    "    tf_df = pd.DataFrame(tf_list,columns=['단어', '빈도'])\n",
    "    return tf_df\n",
    "\n",
    "def save_to_csv(keyword, tf_df):\n",
    "    # save to csv\n",
    "    filename = \"./output/\" + \"최빈어_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "    # filename_list.append(filename)\n",
    "    tf_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-validation",
   "metadata": {
    "papermill": {
     "duration": 0.034432,
     "end_time": "2021-03-23T05:00:02.908362",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.873930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interested_words():\n",
    "    # 핵심단어 읽어 오기\n",
    "    my_sheet = '소비키워드'\n",
    "    keywords_filename = 'deskresearch_.xlsx'\n",
    "    df = read_excel(keywords_filename, sheet_name = my_sheet, header=1) # index_col='번호'\n",
    "    keywords = df['핵심단어']\n",
    "    subkeywords = df['대체어']\n",
    "    interested_words = df['키워드']\n",
    "    return keywords, subkeywords, interested_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-section",
   "metadata": {
    "papermill": {
     "duration": 0.078265,
     "end_time": "2021-03-23T05:00:03.003492",
     "exception": false,
     "start_time": "2021-03-23T05:00:02.925227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "searchkeys = []\n",
    "keywords, subkeywords, interested_words = interested_words()\n",
    "\n",
    "for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "    subkeyword = subkeyword.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "    interested_word = subkeyword + \"|\" + interested_word.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "    searchkey = keyword + \"|\" + subkeyword.replace(\",\",\"|\") + \"|\" + interested_word\n",
    "    searchkeys.append(searchkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-december",
   "metadata": {
    "papermill": {
     "duration": 0.034852,
     "end_time": "2021-03-23T05:00:03.049271",
     "exception": false,
     "start_time": "2021-03-23T05:00:03.014419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "searchkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-depression",
   "metadata": {
    "papermill": {
     "duration": 0.027342,
     "end_time": "2021-03-23T05:00:03.094027",
     "exception": false,
     "start_time": "2021-03-23T05:00:03.066685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for searchkey in searchkeys:\n",
    "    lst.append(searchkey.replace(\" \",\"\").split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-heater",
   "metadata": {
    "papermill": {
     "duration": 0.024268,
     "end_time": "2021-03-23T05:00:03.135238",
     "exception": false,
     "start_time": "2021-03-23T05:00:03.110970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def oneDArray(x):\n",
    "    return list(itertools.chain(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-oracle",
   "metadata": {
    "papermill": {
     "duration": 0.028069,
     "end_time": "2021-03-23T05:00:03.177296",
     "exception": false,
     "start_time": "2021-03-23T05:00:03.149227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst = oneDArray(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-mainland",
   "metadata": {
    "papermill": {
     "duration": 0.13735,
     "end_time": "2021-03-23T05:00:03.328841",
     "exception": false,
     "start_time": "2021-03-23T05:00:03.191491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for w in lst:\n",
    "    print (w + \"\\t\" + \"ncn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-victoria",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-03-23T05:00:03.348363",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "    \n",
    "    df = p.readall(keyword.replace(\" \",\"\"))\n",
    "    df = df[ (df['date'] >= '2019-07-01') & (df['date'] < '2020-07-01')]\n",
    "    df = df.drop_duplicates()\n",
    "    # print(df.shape)  \n",
    "    rows = df['title'].apply(preprocessing) + df['content'].apply(preprocessing)\n",
    "    # print(len(rows), type(rows))\n",
    "    sentences = preprocessing_2(rows)\n",
    "    # print(len(sentences), type(sentences))\n",
    "    corpus = preprocessing_3(sentences)\n",
    "    # print(len(corpus), type(corpus))\n",
    "    \n",
    "    tf_df = preprocessing_4(corpus)\n",
    "    save_to_csv(keyword, tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-person",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/python-nlp-e2e/0002-월간식당_최빈어.ipynb",
   "output_path": "/home/jovyan/python-nlp-e2e/0002-월간식당_최빈어.ipynb",
   "parameters": {},
   "start_time": "2021-03-23T04:59:54.572996",
   "version": "2.3.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
