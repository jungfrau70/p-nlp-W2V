{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월간식당 빅데이터 분석\n",
    "\n",
    "* 키워드별 단어 출현 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "from pandas import read_excel\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from msba import posts as p\n",
    "from msba import stopwords as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interested_words():\n",
    "    # 핵심단어 읽어 오기\n",
    "    my_sheet = '소비키워드'\n",
    "    keywords_filename = 'deskresearch_.xlsx'\n",
    "    df = read_excel(keywords_filename, sheet_name = my_sheet, header=1) # index_col='번호'\n",
    "    keywords = df['핵심단어']\n",
    "    subkeywords = df['대체어']\n",
    "    interested_words = df['키워드']\n",
    "    return keywords, subkeywords, interested_words\n",
    "\n",
    "def preprocessing(text):\n",
    "    # 개행문자 제거\n",
    "    text = re.sub('\\\\\\\\n', ' ', text)\n",
    "    text = re.sub('[A-z]', '', text)\n",
    "#     text = re.sub('[0-9]', '', text)\n",
    "    text = re.sub('[\\'\\\"]', '', text)\n",
    "    res = ''.join([i for i in text if not i.isdigit()]) \n",
    "    return text\n",
    "\n",
    "def preprocessing_2(rows):\n",
    "    pattern = re.compile(r\"[.,?!★~]\")\n",
    "    sentences = []\n",
    "    for row in rows:\n",
    "        sentences = sentences + pattern.split(row.replace(\"\\xa0\", \"\").replace(\"\\t\",\"\").strip())\n",
    "        sentences = list(set(sentences))        \n",
    "    return sentences\n",
    "\n",
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)\n",
    "\n",
    "# 실전 : 말뭉치 생성 및 전처리\n",
    "def preprocessing_3(sentences):\n",
    "    # Convert list of strings to string\n",
    "#     full_str = convert_list_to_string(sentences[1])\n",
    "#     # print(full_str)\n",
    "\n",
    "    pattern = re.compile(r\".*(광고정보).*\")\n",
    "\n",
    "    corpus = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence):\n",
    "            sentence = sentence.strip().split(\" \")\n",
    "            sentence = convert_list_to_string(sentence).replace(\"  \", \" \").strip()\n",
    "            if pattern.match(sentence):\n",
    "                continue\n",
    "            else:\n",
    "                corpus.append(sentence)\n",
    "    return corpus\n",
    "\n",
    "def preprocessing_4(docs):\n",
    "    # docs = [\n",
    "    #         w for w in hannanum.nouns(\" \".join(cell)) for cell in cells\n",
    "    #         if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))\n",
    "    # ]\n",
    "    vect = CountVectorizer(stop_words=stopwords.stopwords_kr, min_df=5, max_df=200).fit(docs)\n",
    "    count = vect.transform(docs).toarray().sum(axis=0)\n",
    "    idx = np.argsort(-count)\n",
    "    count = count[idx]\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]\n",
    "    # plt.bar(range(len(count)), count)\n",
    "    # plt.show()\n",
    "\n",
    "    tf_list = list(zip(feature_name, count))[:100]\n",
    "\n",
    "    tf_df = pd.DataFrame(tf_list,columns=['단어', '빈도'])\n",
    "    return tf_df\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    result = \"\"\n",
    "    word_tokens = sentence.split(' ')\n",
    "    for w in word_tokens: \n",
    "        if w not in stopwords.stopwords_kr: \n",
    "            result = result + w + \" \"\n",
    "    # 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "    # result=[word for word in word_tokens if not word in stop_words]\n",
    "    return result\n",
    "\n",
    "def save_to_csv(keyword, tf_df):\n",
    "    # save to csv\n",
    "    filename = \"./output/\" + \"유관어_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "    # filename_list.append(filename)\n",
    "    tf_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드/관심어 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords, subkeywords, interested_words = interested_words()\n",
    "\n",
    "for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "    subkeyword = subkeyword.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "    interested_word = subkeyword + \"|\" + interested_word.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "#     print(keyword, \" : \" , subkeyword, interested_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자신의 관심사에 맞는 단어로 데이터 가져오기"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 관심 기간 : 2019.7.1 ~ 2019.6.30\n",
    "### 추가 필터 : 핵심단어 별 관심단어\n",
    "### 레코드 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "# #     keyword = '1인미디어'\n",
    "#     df = p.readall(keyword.replace(\" \",\"\"))\n",
    "#     df = df[ (df['date'] >= '2019-07-01') & (df['date'] < '2020-07-01')]\n",
    "#     df = df.drop_duplicates()\n",
    "#     # print(df.shape)  \n",
    "#     rows = df['title'].apply(preprocessing) + df['content'].apply(preprocessing)\n",
    "#     # print(len(rows), type(rows))\n",
    "#     sentences = preprocessing_2(rows)\n",
    "#     # print(len(sentences), type(sentences))\n",
    "#     sentences = preprocessing_3(sentences)\n",
    "#     # print(len(corpus), type(corpus))\n",
    "\n",
    "#     dataset = []\n",
    "#     for sentence in sentences:\n",
    "#         sentence = remove_stopwords(sentence)\n",
    "#         dataset.append(kkma.nouns(sentence))\n",
    "\n",
    "#     dataset = [[y for y in x if not len(y)==1] for x in dataset] # 한글자 문장 제거\n",
    "#     dataset = [[y for y in x if not y.isdigit()] for x in dataset] # 숫자로된 문장 제거\n",
    "#     #     dataset[:10]\n",
    "\n",
    "#     # 모형 구축\n",
    "#     model = Word2Vec(dataset,            # 불용어 처리한 후 \n",
    "#                     sg=1,                # skip-gram 적용: 중심 단어로 주변 단어를 예측\n",
    "#                     window=5,            # 중심 단어로부터 좌우 5개 단어까지 학습에 적용\n",
    "#                     min_count=1          # 전체문서에서 최소 1회 이상 출현단어로 학습 진행             \n",
    "#                     )\n",
    "#     model.init_sims(replace=True)\n",
    "\n",
    "#     w2c = dict()\n",
    "#     for item in model.wv.vocab:\n",
    "#         w2c[item]=model.wv.vocab[item].count\n",
    "\n",
    "#     w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "# #     w2cSorted\n",
    "# #     type(w2cSorted)\n",
    "\n",
    "#     # 가장 유사한 단어 100개\n",
    "#     df_co = pd.DataFrame(model.wv.most_similar(keyword, topn=100), columns=['단어', '유사도'])\n",
    "#     # 100개 보기\n",
    "# #     df_co.head(100)\n",
    "#     save_to_csv(keyword, df_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "keyword = '1인미디어'\n",
    "df = p.readall(keyword.replace(\" \",\"\"))\n",
    "df = df[ (df['date'] >= '2019-07-01') & (df['date'] < '2020-07-01')]\n",
    "df = df.drop_duplicates()\n",
    "# print(df.shape)  \n",
    "rows = df['title'].apply(preprocessing) + df['content'].apply(preprocessing)\n",
    "# print(len(rows), type(rows))\n",
    "sentences = preprocessing_2(rows)\n",
    "# print(len(sentences), type(sentences))\n",
    "sentences = preprocessing_3(sentences)\n",
    "# print(len(corpus), type(corpus))\n",
    "\n",
    "dataset = []\n",
    "for sentence in sentences:\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    dataset.append(kkma.nouns(sentence))\n",
    "\n",
    "dataset = [[y for y in x if not len(y)==1] for x in dataset] # 한글자 문장 제거\n",
    "dataset = [[y for y in x if not y.isdigit()] for x in dataset] # 숫자로된 문장 제거\n",
    "#     dataset[:10]\n",
    "\n",
    "# 모형 구축\n",
    "model = Word2Vec(dataset,            # 불용어 처리한 후 \n",
    "                sg=1,                # skip-gram 적용: 중심 단어로 주변 단어를 예측\n",
    "                window=5,            # 중심 단어로부터 좌우 5개 단어까지 학습에 적용\n",
    "                min_count=1          # 전체문서에서 최소 1회 이상 출현단어로 학습 진행             \n",
    "                )\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "w2c = dict()\n",
    "for item in model.wv.vocab:\n",
    "    w2c[item]=model.wv.vocab[item].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "w2cSorted\n",
    "#     type(w2cSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 유사한 단어 100개\n",
    "df_co = pd.DataFrame(model.wv.most_similar(keyword, topn=100), columns=['단어', '유사도'])\n",
    "# 100개 보기\n",
    "df_co.head(100)\n",
    "# save_to_csv(keyword, df_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def displayWordCloud(data = None, backgroundcolor = 'white', width=800, height=600 ):\n",
    "    wordcloud = WordCloud(\n",
    "                        font_path = './Library/Fonts/NanumBarunGothic.ttf', \n",
    "                        stopwords = stopwords_kr,\n",
    "                        background_color = backgroundcolor, \n",
    "                         width = width, height = height).generate(data)\n",
    "    plt.figure(figsize = (15 , 10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(keyword)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.noun import LRNounExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "noun_extractor = LRNounExtractor(verbose=True)\n",
    "noun_extractor.train(sentences)\n",
    "nouns = noun_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출된 명사를 찍어봅니다.\n",
    "%time displayWordCloud(' '.join(nouns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
