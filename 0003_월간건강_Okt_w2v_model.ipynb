{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bridal-angel",
   "metadata": {
    "papermill": {
     "duration": 0.011072,
     "end_time": "2021-03-23T05:10:11.071392",
     "exception": false,
     "start_time": "2021-03-23T05:10:11.060320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 키워드별 단어 출현 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "funky-cuisine",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-23T05:10:11.100805Z",
     "iopub.status.busy": "2021-03-23T05:10:11.099888Z",
     "iopub.status.idle": "2021-03-23T05:10:11.105349Z",
     "shell.execute_reply": "2021-03-23T05:10:11.104481Z"
    },
    "papermill": {
     "duration": 0.025015,
     "end_time": "2021-03-23T05:10:11.105623",
     "exception": false,
     "start_time": "2021-03-23T05:10:11.080608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install gensim wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-collaboration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-23T05:10:11.155219Z",
     "iopub.status.busy": "2021-03-23T05:10:11.144866Z",
     "iopub.status.idle": "2021-03-23T05:10:13.395724Z",
     "shell.execute_reply": "2021-03-23T05:10:13.396917Z"
    },
    "papermill": {
     "duration": 2.276346,
     "end_time": "2021-03-23T05:10:13.397098",
     "exception": false,
     "start_time": "2021-03-23T05:10:11.120752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas import read_excel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from msba import posts as p\n",
    "from msba import stopwords as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lasting-lesbian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-23T05:10:13.437420Z",
     "iopub.status.busy": "2021-03-23T05:10:13.430525Z",
     "iopub.status.idle": "2021-03-23T05:10:13.440624Z",
     "shell.execute_reply": "2021-03-23T05:10:13.439634Z"
    },
    "papermill": {
     "duration": 0.036855,
     "end_time": "2021-03-23T05:10:13.440820",
     "exception": false,
     "start_time": "2021-03-23T05:10:13.403965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interested_words():\n",
    "    # 핵심단어 읽어 오기\n",
    "    my_sheet = '소비키워드'\n",
    "    keywords_filename = 'deskresearch_.xlsx'\n",
    "    df = read_excel(keywords_filename, sheet_name = my_sheet, header=1) # index_col='번호'\n",
    "    keywords = df['핵심단어']\n",
    "    subkeywords = df['대체어']\n",
    "    interested_words = df['키워드']\n",
    "    return keywords, subkeywords, interested_words\n",
    "\n",
    "def oneDArray(x):\n",
    "    return list(itertools.chain(*x))\n",
    "\n",
    "def preprocessing(text):\n",
    "    # 개행문자 제거\n",
    "    text = re.sub('\\\\\\\\n', ' ', text)\n",
    "#     text = re.sub('[A-z]', '', text)\n",
    "#     text = re.sub('[0-9]', '', text)\n",
    "    text = re.sub('[\\'\\\"]', '', text)\n",
    "    text = re.sub('[\\'\\n\"]', '', text)\n",
    "    res = ''.join([i for i in text if not i.isdigit()]) \n",
    "    return text\n",
    "\n",
    "def preprocessing_2(rows):\n",
    "    pattern = re.compile(r\"[.,?!★~]\")\n",
    "    sentences = []\n",
    "    for row in rows:\n",
    "        sentences = sentences + pattern.split(row.replace(\"\\xa0\", \"\").replace(\"\\t\",\"\").strip())\n",
    "        sentences = list(set(sentences))        \n",
    "    return sentences\n",
    "\n",
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)\n",
    "\n",
    "# 실전 : 말뭉치 생성 및 전처리\n",
    "def preprocessing_3(sentences):\n",
    "    # Convert list of strings to string\n",
    "#     full_str = convert_list_to_string(sentences[1])\n",
    "#     # print(full_str)\n",
    "\n",
    "    pattern = re.compile(r\".*(광고정보).*\")\n",
    "\n",
    "    corpus = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence):\n",
    "            sentence = sentence.strip().split(\" \")\n",
    "            sentence = convert_list_to_string(sentence).replace(\"  \", \" \").strip()\n",
    "            if pattern.match(sentence):\n",
    "                continue\n",
    "            else:\n",
    "                corpus.append(sentence)\n",
    "    return corpus\n",
    "\n",
    "def preprocessing_4(docs):\n",
    "    # docs = [\n",
    "    #         w for w in hannanum.nouns(\" \".join(cell)) for cell in cells\n",
    "    #         if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))\n",
    "    # ]\n",
    "    vect = CountVectorizer(stop_words=stopwords.stopwords_kr, min_df=5, max_df=200).fit(docs)\n",
    "    count = vect.transform(docs).toarray().sum(axis=0)\n",
    "    idx = np.argsort(-count)\n",
    "    count = count[idx]\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]\n",
    "    # plt.bar(range(len(count)), count)\n",
    "    # plt.show()\n",
    "\n",
    "    tf_list = list(zip(feature_name, count))[:100]\n",
    "\n",
    "    tf_df = pd.DataFrame(tf_list,columns=['단어', '빈도'])\n",
    "    return tf_df\n",
    "\n",
    "def remove_stopwords_from_list(lst):\n",
    "    result = []\n",
    "    for w in lst:\n",
    "        if ((len(w) > 1) and (not w.isdigit())):\n",
    "            if w not in stopwords.stopwords_kr: \n",
    "                result.append(w)\n",
    "    return result\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    result = \"\"\n",
    "    for w in line.split(\" \"):\n",
    "        if ((len(w) > 1) and (not w.isdigit())):\n",
    "            if w not in stopwords.stopwords_kr: \n",
    "                result = result + w + \" \"\n",
    "    return result.strip()\n",
    "\n",
    "def save_to_csv(keyword, output, tf_df):\n",
    "    # save to csv    \n",
    "    filename = \"./output/\" + output + \"_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "    # filename_list.append(filename)\n",
    "    tf_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "empirical-sculpture",
   "metadata": {
    "papermill": {
     "duration": 0.007097,
     "end_time": "2021-03-23T05:10:13.457033",
     "exception": false,
     "start_time": "2021-03-23T05:10:13.449936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "키워드/관심어 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regional-cache",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-23T05:10:13.481181Z",
     "iopub.status.busy": "2021-03-23T05:10:13.480231Z",
     "iopub.status.idle": "2021-03-23T05:10:13.793252Z",
     "shell.execute_reply": "2021-03-23T05:10:13.792207Z"
    },
    "papermill": {
     "duration": 0.328597,
     "end_time": "2021-03-23T05:10:13.793504",
     "exception": false,
     "start_time": "2021-03-23T05:10:13.464907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywords, subkeywords, interested_words = interested_words()\n",
    "\n",
    "for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "    subkeyword = subkeyword.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "    interested_word = subkeyword + \"|\" + interested_word.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "#     print(keyword, \" : \" , subkeyword, interested_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-shock",
   "metadata": {
    "papermill": {
     "duration": 0.009741,
     "end_time": "2021-03-23T05:10:13.815053",
     "exception": false,
     "start_time": "2021-03-23T05:10:13.805312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 자신의 관심사에 맞는 단어로 데이터 가져오기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "daily-imaging",
   "metadata": {
    "papermill": {
     "duration": 0.013615,
     "end_time": "2021-03-23T05:10:13.845316",
     "exception": false,
     "start_time": "2021-03-23T05:10:13.831701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 관심 기간 : 2019.7.1 ~ 2019.6.30\n",
    "### 추가 필터 : 핵심단어 별 관심단어\n",
    "### 레코드 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binary-pricing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-23T05:10:13.893738Z",
     "iopub.status.busy": "2021-03-23T05:10:13.878378Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-03-23T05:10:13.864110",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 35366\n",
      "Num of words - 200682\n"
     ]
    }
   ],
   "source": [
    "# %%time|\n",
    "for keyword, subkeyword, interested_word in tqdm(zip(keywords, subkeywords, interested_words)):\n",
    "\n",
    "    #     keyword = '1인 외식'\n",
    "    # keyword = keyword.replace(\" \",\"\")\n",
    "    df = p.readall(keyword.replace(\" \",\"\"))\n",
    "    df = df[ (df['date'] >= '2019-07-01') & (df['date'] < '2020-07-01')]\n",
    "    df = df.drop_duplicates()\n",
    "    # print(df.shape)  \n",
    "    rows = df['title'].apply(preprocessing) + df['content'].apply(preprocessing)\n",
    "    # print(len(rows), type(rows))\n",
    "    sentences = preprocessing_2(rows)\n",
    "    # print(len(sentences), type(sentences))\n",
    "    sentences = preprocessing_3(sentences)\n",
    "    # print(len(corpus), type(corpus))\n",
    "\n",
    "    # 4. 각 문장별로 형태소 구분하기\n",
    "    sentences_tag = []\n",
    "    for sentence in sentences:\n",
    "        morph = okt.pos(sentence)\n",
    "        sentences_tag.append(morph)\n",
    "    #     print(morph)\n",
    "    #     print('-'*30)\n",
    "\n",
    "    # print(sentences_tag)\n",
    "    # print(len(sentences_tag))\n",
    "    # print('\\n'*3)\n",
    "\n",
    "#     sentences_tag\n",
    "\n",
    "    # 5. 명사 혹은 형용사인 품사만 선별해 리스트에 담기\n",
    "    noun_adj_list = []\n",
    "\n",
    "    noun_adj_list = []\n",
    "    for sentence1 in sentences_tag:\n",
    "        words = \"\"\n",
    "        for word, tag in sentence1:\n",
    "            if tag in ['Noun','Adjective']:\n",
    "                words = words + word + \" \"\n",
    "    #     print(words)\n",
    "    #     print(\"------------\")\n",
    "        words = remove_stopwords(words)\n",
    "        noun_adj_list.append(words)\n",
    "\n",
    "    #             line = remove_stopwords(line)\n",
    "    #         print(line)\n",
    "    #     noun_adj_list.append(line)\n",
    "\n",
    "#     noun_adj_list\n",
    "\n",
    "    #########################################################################\n",
    "    # 최빈어\n",
    "    #########################################################################\n",
    "    # import itertools\n",
    "\n",
    "    # def oneDArray(x):\n",
    "    #     return list(itertools.chain(*x))\n",
    "\n",
    "    tf_list = []\n",
    "    for sentence1 in noun_adj_list:\n",
    "        sentence1.split()\n",
    "        tf_list.append(sentence1.split())\n",
    "\n",
    "#     type(tf_list)\n",
    "\n",
    "    tf_list = oneDArray(tf_list)\n",
    "    \n",
    "#     tf_list\n",
    "\n",
    "    # 6. 선별된 품사별 빈도수 계산 & 상위 빈도 10위 까지 출력\n",
    "    counts = Counter(tf_list)\n",
    "\n",
    "    toplist = counts.most_common(200)\n",
    "\n",
    "    # type(toplist)\n",
    "\n",
    "    tf_df = pd.DataFrame (toplist,columns=['단어', '빈도수'])\n",
    "    save_to_csv(keyword, \"최빈어\", tf_df)\n",
    "    \n",
    "   \n",
    "    #########################################################################\n",
    "    # 유사도\n",
    "    #########################################################################\n",
    "    # using remove() to \n",
    "    # perform removal \n",
    "    while(\"\" in noun_adj_list) : \n",
    "        noun_adj_list.remove(\"\") \n",
    "\n",
    "    dataset = pd.DataFrame(noun_adj_list, columns=['문장'])\n",
    "\n",
    "    nan_value = float(\"NaN\")\n",
    "    dataset.replace(\"\", nan_value, inplace=True)\n",
    "    dataset.dropna(subset = [\"문장\"], inplace=True)\n",
    "    # dataset.reindex\n",
    "\n",
    "    # noun_adj_list[12]\n",
    "    # dataset['문장'][13]\n",
    "\n",
    "    tmp_corpus = dataset['문장'].map(lambda x: x.split('.'))\n",
    "    # tmp_corpus\n",
    "\n",
    "    # type(tmp_corpus)\n",
    "    # tmp_corpus[11]\n",
    "    # tmp_corpus[13]\n",
    "\n",
    "    # corpus [[w1,w2,w3..],[..]]\n",
    "    corpus = []\n",
    "    for i in range(len(tmp_corpus)):\n",
    "        for line in tmp_corpus[i]:\n",
    "    #         print(i)\n",
    "            words = [x for x in line.split()]\n",
    "            corpus.append(words)\n",
    "\n",
    "#     corpus\n",
    "\n",
    "    num_of_sentences = len(corpus)\n",
    "    num_of_words = 0\n",
    "    for line in corpus:\n",
    "        num_of_words += len(line)\n",
    "\n",
    "    print('Num of sentences - %s'%(num_of_sentences))\n",
    "    print('Num of words - %s'%(num_of_words))\n",
    "\n",
    "    phrases = Phrases(sentences=corpus,min_count=25,threshold=50)\n",
    "    bigram = Phraser(phrases)\n",
    "\n",
    "    for index,sentence in enumerate(corpus):\n",
    "        corpus[index] = bigram[sentence]\n",
    "\n",
    "    # shuffle corpus\n",
    "    def shuffle_corpus(sentences):\n",
    "        shuffled = list(sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled\n",
    "\n",
    "    # sg - skip gram |  window = size of the window | size = vector dimension\n",
    "    size = 100\n",
    "    window_size = 5 # sentences weren't too long, so\n",
    "    epochs = 100\n",
    "    min_count = 2\n",
    "    workers = 4\n",
    "\n",
    "    # train word2vec model using gensim\n",
    "    # model = Word2Vec(corpus, sg=1,window=window_size,size=size,\n",
    "    #                  min_count=min_count,workers=workers,iter=epochs,sample=0.01)\n",
    "\n",
    "#     %%time\n",
    "    model = Word2Vec(dataset,            # 불용어 처리한 후 \n",
    "                    sg=2,                # skip-gram 적용: 중심 단어로 주변 단어를 예측\n",
    "                    window=window_size,  # 중심 단어로부터 좌우 5개 단어까지 학습에 적용\n",
    "                    iter=epochs,\n",
    "                    #workers=workers,\n",
    "                    #size=size,\n",
    "                    sample=0.01,\n",
    "                    min_count=1          # 전체문서에서 최소 1회 이상 출현단어로 학습 진행             \n",
    "                    )\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    model.build_vocab(sentences=shuffle_corpus(corpus),update=True)\n",
    "\n",
    "    for i in trange(5):\n",
    "        model.train(sentences=shuffle_corpus(corpus),epochs=50,total_examples=model.corpus_count)\n",
    "\n",
    "#     corpus\n",
    "\n",
    "    # save model\n",
    "    modelname = \"./model/\" + \"w2v_\" + keyword.replace(\" \",\"\")  \n",
    "    model.save(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-action",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/python-nlp-e2e/0003_월간건강_Okt_w2v_model.ipynb",
   "output_path": "/home/jovyan/python-nlp-e2e/0003_월간건강_Okt_w2v_model.ipynb",
   "parameters": {},
   "start_time": "2021-03-23T05:10:10.132405",
   "version": "2.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}