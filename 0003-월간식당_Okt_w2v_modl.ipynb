{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월간식당 빅데이터 분석\n",
    "\n",
    "* 키워드별 단어 출현 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas import read_excel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from msba import posts as p\n",
    "from msba import stopwords as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interested_words():\n",
    "    # 핵심단어 읽어 오기\n",
    "    my_sheet = '소비키워드'\n",
    "    keywords_filename = 'deskresearch_.xlsx'\n",
    "    df = read_excel(keywords_filename, sheet_name = my_sheet, header=1) # index_col='번호'\n",
    "    keywords = df['핵심단어']\n",
    "    subkeywords = df['대체어']\n",
    "    interested_words = df['키워드']\n",
    "    return keywords, subkeywords, interested_words\n",
    "\n",
    "def oneDArray(x):\n",
    "    return list(itertools.chain(*x))\n",
    "\n",
    "def preprocessing(text):\n",
    "    # 개행문자 제거\n",
    "    text = re.sub('\\\\\\\\n', ' ', text)\n",
    "#     text = re.sub('[A-z]', '', text)\n",
    "#     text = re.sub('[0-9]', '', text)\n",
    "    text = re.sub('[\\'\\\"]', '', text)\n",
    "    text = re.sub('[\\'\\n\"]', '', text)\n",
    "    res = ''.join([i for i in text if not i.isdigit()]) \n",
    "    return text\n",
    "\n",
    "def preprocessing_2(rows):\n",
    "    pattern = re.compile(r\"[.,?!★~]\")\n",
    "    sentences = []\n",
    "    for row in rows:\n",
    "        sentences = sentences + pattern.split(row.replace(\"\\xa0\", \"\").replace(\"\\t\",\"\").strip())\n",
    "        sentences = list(set(sentences))        \n",
    "    return sentences\n",
    "\n",
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)\n",
    "\n",
    "# 실전 : 말뭉치 생성 및 전처리\n",
    "def preprocessing_3(sentences):\n",
    "    # Convert list of strings to string\n",
    "#     full_str = convert_list_to_string(sentences[1])\n",
    "#     # print(full_str)\n",
    "\n",
    "    pattern = re.compile(r\".*(광고정보).*\")\n",
    "\n",
    "    corpus = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence):\n",
    "            sentence = sentence.strip().split(\" \")\n",
    "            sentence = convert_list_to_string(sentence).replace(\"  \", \" \").strip()\n",
    "            if pattern.match(sentence):\n",
    "                continue\n",
    "            else:\n",
    "                corpus.append(sentence)\n",
    "    return corpus\n",
    "\n",
    "def preprocessing_4(docs):\n",
    "    # docs = [\n",
    "    #         w for w in hannanum.nouns(\" \".join(cell)) for cell in cells\n",
    "    #         if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))\n",
    "    # ]\n",
    "    vect = CountVectorizer(stop_words=stopwords.stopwords_kr, min_df=5, max_df=200).fit(docs)\n",
    "    count = vect.transform(docs).toarray().sum(axis=0)\n",
    "    idx = np.argsort(-count)\n",
    "    count = count[idx]\n",
    "    feature_name = np.array(vect.get_feature_names())[idx]\n",
    "    # plt.bar(range(len(count)), count)\n",
    "    # plt.show()\n",
    "\n",
    "    tf_list = list(zip(feature_name, count))[:100]\n",
    "\n",
    "    tf_df = pd.DataFrame(tf_list,columns=['단어', '빈도'])\n",
    "    return tf_df\n",
    "\n",
    "def remove_stopwords_from_list(lst):\n",
    "    result = []\n",
    "    for w in lst:\n",
    "        if ((len(w) > 1) and (not w.isdigit())):\n",
    "            if w not in stopwords.stopwords_kr: \n",
    "                result.append(w)\n",
    "    return result\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    result = \"\"\n",
    "    for w in line.split(\" \"):\n",
    "        if ((len(w) > 1) and (not w.isdigit())):\n",
    "            if w not in stopwords.stopwords_kr: \n",
    "                result = result + w + \" \"\n",
    "    return result.strip()\n",
    "\n",
    "def save_to_csv(keyword, output, tf_df):\n",
    "    # save to csv    \n",
    "    filename = \"./output/\" + output + \"_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "    # filename_list.append(filename)\n",
    "    tf_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드/관심어 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords, subkeywords, interested_words = interested_words()\n",
    "\n",
    "for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "    subkeyword = subkeyword.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "    interested_word = subkeyword + \"|\" + interested_word.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "#     print(keyword, \" : \" , subkeyword, interested_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자신의 관심사에 맞는 단어로 데이터 가져오기"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 관심 기간 : 2019.7.1 ~ 2019.6.30\n",
    "### 추가 필터 : 핵심단어 별 관심단어\n",
    "### 레코드 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 35366\n",
      "Num of words - 200682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:24<01:39, 24.81s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:49<01:13, 24.64s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:13<00:48, 24.47s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:37<00:24, 24.50s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:00<00:00, 24.19s/it]\u001b[A\n",
      "1it [04:28, 268.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 41273\n",
      "Num of words - 247511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:32<02:11, 32.92s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:04<01:37, 32.37s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:34<01:03, 31.81s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:05<00:31, 31.56s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:35<00:00, 31.18s/it]\u001b[A\n",
      "2it [09:49, 284.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 14890\n",
      "Num of words - 121135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:18<01:12, 18.20s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:34<00:52, 17.65s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:50<00:34, 17.06s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:05<00:16, 16.61s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:21<00:00, 16.34s/it]\u001b[A\n",
      "3it [12:23, 245.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 30822\n",
      "Num of words - 181760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:23<01:35, 23.92s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:46<01:10, 23.46s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:07<00:45, 22.83s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:29<00:22, 22.42s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:51<00:00, 22.22s/it]\u001b[A\n",
      "4it [16:26, 244.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 36238\n",
      "Num of words - 216080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:28<01:54, 28.51s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:57<01:25, 28.56s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:25<00:56, 28.38s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:52<00:28, 28.09s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:20<00:00, 28.02s/it]\u001b[A\n",
      "5it [21:11, 256.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 23363\n",
      "Num of words - 156434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:19<01:18, 19.65s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:39<00:58, 19.59s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:57<00:38, 19.30s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:15<00:18, 18.77s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:33<00:00, 18.72s/it]\u001b[A\n",
      "6it [24:24, 237.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 37288\n",
      "Num of words - 232770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:30<02:01, 30.28s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:59<01:30, 30.06s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:29<00:59, 29.88s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:58<00:29, 29.59s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:26<00:00, 29.34s/it]\u001b[A\n",
      "7it [29:38, 260.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 42575\n",
      "Num of words - 244789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:29<01:58, 29.64s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:58<01:28, 29.47s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:28<00:58, 29.48s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:57<00:29, 29.42s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:25<00:00, 29.08s/it]\u001b[A\n",
      "8it [35:09, 281.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 28182\n",
      "Num of words - 176676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:21<01:27, 21.82s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:42<01:04, 21.58s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:04<00:43, 21.61s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:26<00:21, 21.67s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:47<00:00, 21.48s/it]\u001b[A\n",
      "9it [38:59, 266.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 15828\n",
      "Num of words - 102422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:13<00:52, 13.18s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:26<00:39, 13.14s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:38<00:25, 12.73s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:49<00:12, 12.43s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:01<00:00, 12.31s/it]\u001b[A\n",
      "10it [41:02, 223.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 36452\n",
      "Num of words - 225356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:28<01:54, 28.75s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:56<01:24, 28.31s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:23<00:55, 27.99s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:50<00:27, 27.64s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:18<00:00, 27.74s/it]\u001b[A\n",
      "11it [45:58, 244.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 30462\n",
      "Num of words - 165123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:20<01:22, 20.61s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:39<01:00, 20.15s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:57<00:39, 19.55s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:16<00:19, 19.39s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:34<00:00, 18.99s/it]\u001b[A\n",
      "12it [49:47, 240.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 10297\n",
      "Num of words - 77239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:11<00:45, 11.25s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:21<00:32, 10.86s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:30<00:20, 10.43s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:40<00:10, 10.13s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:49<00:00,  9.91s/it]\u001b[A\n",
      "13it [51:22, 196.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 3899\n",
      "Num of words - 26135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:03<00:14,  3.53s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:06<00:10,  3.50s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:10<00:06,  3.47s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:13<00:03,  3.51s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.45s/it]\u001b[A\n",
      "14it [51:53, 147.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 20263\n",
      "Num of words - 129208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:17<01:08, 17.05s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:32<00:49, 16.66s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:49<00:33, 16.75s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:05<00:16, 16.58s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:21<00:00, 16.30s/it]\u001b[A\n",
      "15it [54:36, 151.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 25233\n",
      "Num of words - 155293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:19<01:16, 19.17s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:37<00:56, 18.99s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:55<00:37, 18.68s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:13<00:18, 18.47s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:33<00:00, 18.68s/it]\u001b[A\n",
      "16it [57:49, 164.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 17450\n",
      "Num of words - 103628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:14<00:56, 14.24s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:27<00:41, 13.90s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:40<00:27, 13.55s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:52<00:13, 13.10s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:03<00:00, 12.78s/it]\u001b[A\n",
      "17it [59:55, 152.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 38234\n",
      "Num of words - 270564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:37<02:28, 37.11s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:11<01:48, 36.30s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:45<01:11, 35.70s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:19<00:35, 35.18s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:52<00:00, 34.50s/it]\u001b[A\n",
      "18it [1:05:31, 207.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 2349\n",
      "Num of words - 14484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:02<00:11,  2.81s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:05<00:08,  2.80s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:08<00:05,  2.70s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:10<00:02,  2.63s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.56s/it]\u001b[A\n",
      "19it [1:05:54, 152.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 32303\n",
      "Num of words - 203307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:26<01:46, 26.69s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:52<01:19, 26.51s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:18<00:52, 26.31s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:46<00:26, 26.69s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:12<00:00, 26.47s/it]\u001b[A\n",
      "20it [1:10:11, 183.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 33915\n",
      "Num of words - 220404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:26<01:46, 26.53s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:51<01:18, 26.18s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:16<00:51, 25.81s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:41<00:25, 25.48s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:06<00:00, 25.24s/it]\u001b[A\n",
      "21it [1:14:49, 212.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 54426\n",
      "Num of words - 326067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:41<02:47, 41.78s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:22<02:04, 41.52s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [02:02<01:21, 41.00s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:42<00:40, 40.70s/it]\u001b[A\n",
      "100%|██████████| 5/5 [03:21<00:00, 40.38s/it]\u001b[A\n",
      "22it [1:21:34, 269.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 43766\n",
      "Num of words - 271472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:32<02:11, 32.97s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:04<01:37, 32.63s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:35<01:04, 32.16s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:07<00:31, 31.91s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:37<00:00, 31.55s/it]\u001b[A\n",
      "23it [1:27:08, 289.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 27205\n",
      "Num of words - 167685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:19<01:18, 19.62s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:38<00:58, 19.49s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:57<00:38, 19.26s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:16<00:19, 19.23s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:35<00:00, 19.03s/it]\u001b[A\n",
      "24it [1:30:39, 265.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 713\n",
      "Num of words - 4830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:01<00:04,  1.19s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:02<00:03,  1.21s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:03<00:02,  1.20s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:04<00:01,  1.18s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.17s/it]\u001b[A\n",
      "25it [1:30:48, 188.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 41488\n",
      "Num of words - 225039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:25<01:42, 25.67s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:50<01:16, 25.40s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:14<00:50, 25.14s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:39<00:24, 24.90s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:03<00:00, 24.65s/it]\u001b[A\n",
      "26it [1:35:26, 215.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 35122\n",
      "Num of words - 214245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:26<01:46, 26.54s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:51<01:18, 26.13s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:16<00:51, 25.85s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:41<00:25, 25.62s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:06<00:00, 25.34s/it]\u001b[A\n",
      "27it [1:40:02, 233.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 43820\n",
      "Num of words - 281905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:33<02:12, 33.18s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:05<01:39, 33.03s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:37<01:05, 32.67s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:09<00:32, 32.41s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:41<00:00, 32.25s/it]\u001b[A\n",
      "28it [1:45:26, 260.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 27761\n",
      "Num of words - 175754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:22<01:30, 22.54s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:44<01:06, 22.28s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:05<00:43, 21.93s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:25<00:21, 21.48s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:45<00:00, 21.19s/it]\u001b[A\n",
      "29it [1:48:55, 245.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 66905\n",
      "Num of words - 456253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:53<03:35, 53.99s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:46<02:40, 53.45s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [02:39<01:46, 53.35s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [03:31<00:53, 53.13s/it]\u001b[A\n",
      "100%|██████████| 5/5 [04:24<00:00, 52.89s/it]\u001b[A\n",
      "30it [1:57:12, 320.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 43213\n",
      "Num of words - 263589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:31<02:05, 31.46s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:01<01:33, 31.11s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:31<01:01, 30.75s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:01<00:30, 30.61s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:31<00:00, 30.39s/it]\u001b[A\n",
      "31it [2:02:19, 316.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 48566\n",
      "Num of words - 315288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:41<02:44, 41.19s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:21<02:02, 40.79s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [02:00<01:20, 40.25s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:38<00:39, 39.69s/it]\u001b[A\n",
      "100%|██████████| 5/5 [03:16<00:00, 39.27s/it]\u001b[A\n",
      "32it [2:13:27, 422.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 20861\n",
      "Num of words - 137480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:16<01:05, 16.34s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:32<00:48, 16.17s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:47<00:31, 15.85s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:02<00:15, 15.79s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:17<00:00, 15.59s/it]\u001b[A\n",
      "33it [2:16:11, 344.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 19630\n",
      "Num of words - 105647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:12<00:48, 12.18s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:23<00:35, 11.95s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:34<00:23, 11.65s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:45<00:11, 11.40s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:56<00:00, 11.20s/it]\u001b[A\n",
      "34it [2:18:16, 278.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 20445\n",
      "Num of words - 123214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:14<00:59, 14.92s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:29<00:44, 14.68s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:43<00:29, 14.58s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:57<00:14, 14.31s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:10<00:00, 14.10s/it]\u001b[A\n",
      "35it [2:20:44, 241.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "for keyword, subkeyword, interested_word in tqdm(zip(keywords, subkeywords, interested_words)):\n",
    "\n",
    "    #     keyword = '1인 외식'\n",
    "    # keyword = keyword.replace(\" \",\"\")\n",
    "    df = p.readall(keyword.replace(\" \",\"\"))\n",
    "    df = df[ (df['date'] >= '2019-07-01') & (df['date'] < '2020-07-01')]\n",
    "    df = df.drop_duplicates()\n",
    "    # print(df.shape)  \n",
    "    rows = df['title'].apply(preprocessing) + df['content'].apply(preprocessing)\n",
    "    # print(len(rows), type(rows))\n",
    "    sentences = preprocessing_2(rows)\n",
    "    # print(len(sentences), type(sentences))\n",
    "    sentences = preprocessing_3(sentences)\n",
    "    # print(len(corpus), type(corpus))\n",
    "\n",
    "    # 4. 각 문장별로 형태소 구분하기\n",
    "    sentences_tag = []\n",
    "    for sentence in sentences:\n",
    "        morph = okt.pos(sentence)\n",
    "        sentences_tag.append(morph)\n",
    "    #     print(morph)\n",
    "    #     print('-'*30)\n",
    "\n",
    "    # print(sentences_tag)\n",
    "    # print(len(sentences_tag))\n",
    "    # print('\\n'*3)\n",
    "\n",
    "#     sentences_tag\n",
    "\n",
    "    # 5. 명사 혹은 형용사인 품사만 선별해 리스트에 담기\n",
    "    noun_adj_list = []\n",
    "\n",
    "    noun_adj_list = []\n",
    "    for sentence1 in sentences_tag:\n",
    "        words = \"\"\n",
    "        for word, tag in sentence1:\n",
    "            if tag in ['Noun','Adjective']:\n",
    "                words = words + word + \" \"\n",
    "    #     print(words)\n",
    "    #     print(\"------------\")\n",
    "        words = remove_stopwords(words)\n",
    "        noun_adj_list.append(words)\n",
    "\n",
    "    #             line = remove_stopwords(line)\n",
    "    #         print(line)\n",
    "    #     noun_adj_list.append(line)\n",
    "\n",
    "#     noun_adj_list\n",
    "\n",
    "    #########################################################################\n",
    "    # 최빈어\n",
    "    #########################################################################\n",
    "    # import itertools\n",
    "\n",
    "    # def oneDArray(x):\n",
    "    #     return list(itertools.chain(*x))\n",
    "\n",
    "    tf_list = []\n",
    "    for sentence1 in noun_adj_list:\n",
    "        sentence1.split()\n",
    "        tf_list.append(sentence1.split())\n",
    "\n",
    "#     type(tf_list)\n",
    "\n",
    "    tf_list = oneDArray(tf_list)\n",
    "    \n",
    "#     tf_list\n",
    "\n",
    "    # 6. 선별된 품사별 빈도수 계산 & 상위 빈도 10위 까지 출력\n",
    "    counts = Counter(tf_list)\n",
    "\n",
    "    toplist = counts.most_common(200)\n",
    "\n",
    "    # type(toplist)\n",
    "\n",
    "    tf_df = pd.DataFrame (toplist,columns=['단어', '빈도수'])\n",
    "    save_to_csv(keyword, \"최빈어\", tf_df)\n",
    "    \n",
    "   \n",
    "    #########################################################################\n",
    "    # 유사도\n",
    "    #########################################################################\n",
    "    # using remove() to \n",
    "    # perform removal \n",
    "    while(\"\" in noun_adj_list) : \n",
    "        noun_adj_list.remove(\"\") \n",
    "\n",
    "    dataset = pd.DataFrame(noun_adj_list, columns=['문장'])\n",
    "\n",
    "    nan_value = float(\"NaN\")\n",
    "    dataset.replace(\"\", nan_value, inplace=True)\n",
    "    dataset.dropna(subset = [\"문장\"], inplace=True)\n",
    "    # dataset.reindex\n",
    "\n",
    "    # noun_adj_list[12]\n",
    "    # dataset['문장'][13]\n",
    "\n",
    "    tmp_corpus = dataset['문장'].map(lambda x: x.split('.'))\n",
    "    # tmp_corpus\n",
    "\n",
    "    # type(tmp_corpus)\n",
    "    # tmp_corpus[11]\n",
    "    # tmp_corpus[13]\n",
    "\n",
    "    # corpus [[w1,w2,w3..],[..]]\n",
    "    corpus = []\n",
    "    for i in range(len(tmp_corpus)):\n",
    "        for line in tmp_corpus[i]:\n",
    "    #         print(i)\n",
    "            words = [x for x in line.split()]\n",
    "            corpus.append(words)\n",
    "\n",
    "#     corpus\n",
    "\n",
    "    num_of_sentences = len(corpus)\n",
    "    num_of_words = 0\n",
    "    for line in corpus:\n",
    "        num_of_words += len(line)\n",
    "\n",
    "    print('Num of sentences - %s'%(num_of_sentences))\n",
    "    print('Num of words - %s'%(num_of_words))\n",
    "\n",
    "    phrases = Phrases(sentences=corpus,min_count=25,threshold=50)\n",
    "    bigram = Phraser(phrases)\n",
    "\n",
    "    for index,sentence in enumerate(corpus):\n",
    "        corpus[index] = bigram[sentence]\n",
    "\n",
    "    # shuffle corpus\n",
    "    def shuffle_corpus(sentences):\n",
    "        shuffled = list(sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled\n",
    "\n",
    "    # sg - skip gram |  window = size of the window | size = vector dimension\n",
    "    size = 100\n",
    "    window_size = 5 # sentences weren't too long, so\n",
    "    epochs = 100\n",
    "    min_count = 2\n",
    "    workers = 4\n",
    "\n",
    "    # train word2vec model using gensim\n",
    "    # model = Word2Vec(corpus, sg=1,window=window_size,size=size,\n",
    "    #                  min_count=min_count,workers=workers,iter=epochs,sample=0.01)\n",
    "\n",
    "#     %%time\n",
    "    model = Word2Vec(dataset,            # 불용어 처리한 후 \n",
    "                    sg=2,                # skip-gram 적용: 중심 단어로 주변 단어를 예측\n",
    "                    window=window_size,  # 중심 단어로부터 좌우 5개 단어까지 학습에 적용\n",
    "                    iter=epochs,\n",
    "                    #workers=workers,\n",
    "                    #size=size,\n",
    "                    sample=0.01,\n",
    "                    min_count=1          # 전체문서에서 최소 1회 이상 출현단어로 학습 진행             \n",
    "                    )\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    model.build_vocab(sentences=shuffle_corpus(corpus),update=True)\n",
    "\n",
    "    for i in trange(5):\n",
    "        model.train(sentences=shuffle_corpus(corpus),epochs=50,total_examples=model.corpus_count)\n",
    "\n",
    "#     corpus\n",
    "\n",
    "    # save model\n",
    "    modelname = \"./model/\" + \"w2v_\" + keyword.replace(\" \",\"\")  \n",
    "    model.save(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
