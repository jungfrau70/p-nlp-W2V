{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# customized KoNLPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lovit/customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "from collections import Counter\n",
    "from pandas import read_excel\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "### Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from hanpre import funcs as hp\n",
    "from hanpre import stopwords as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interested_words():\n",
    "    # 핵심단어 읽어 오기\n",
    "    my_sheet = '소비키워드'\n",
    "    keywords_filename = 'deskresearch_.xlsx'\n",
    "    df = read_excel(keywords_filename, sheet_name = my_sheet, header=1) # index_col='번호'\n",
    "    keywords = df['핵심단어']\n",
    "    subkeywords = df['대체어']\n",
    "    interested_words = df['키워드']\n",
    "    return keywords, subkeywords, interested_words\n",
    "\n",
    "my_weights = [\n",
    "    ('num_nouns', -0.1),\n",
    "    ('num_words', -0.2),\n",
    "    ('no_noun', -1),\n",
    "    ('len_sum_of_nouns', 0.2)\n",
    "]\n",
    "\n",
    "def my_evaluate_function(candidate):\n",
    "    num_nouns = len([word for word, pos, begin, e in candidate if pos == 'Noun'])\n",
    "    num_words = len(candidate)\n",
    "    has_no_nouns = (num_nouns == 0)\n",
    "    len_sum_of_nouns = 0 if has_no_nouns else sum(\n",
    "        (len(word) for word, pos, _, _ in candidate if pos == 'Noun'))\n",
    "\n",
    "    scores = (num_nouns, num_words, has_no_nouns, len_sum_of_nouns)\n",
    "    score = sum((score * weight for score, (_, weight) in zip(scores, my_weights)))\n",
    "    return score\n",
    "\n",
    "def oneDArray(x):\n",
    "    return list(itertools.chain(*x))\n",
    "\n",
    "def getTopics(model):\n",
    "    topics = []\n",
    "    for topic in model.print_topics(num_words=500):\n",
    "        i=1\n",
    "        model_words=[]\n",
    "        topic_words=str(topic).split('\"')\n",
    "        for words in topic_words:\n",
    "            if i%2==0:\n",
    "                model_words.append(words)\n",
    "            i+=1\n",
    "        topics.append(model_words)\n",
    "    return topics\n",
    "\n",
    "def save_to_csv(output, keyword, type, tf_df):\n",
    "    # save to csv    \n",
    "    filename = \"./output/\" + output + \"_\" + type + \"_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "    # filename_list.append(filename)\n",
    "    tf_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "keywords, subkeywords, interested_words = interested_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.444 Gbory 0.294 Gb\n",
      "all cohesion probabilities was computed. # words = 29547\n",
      "all branching entropies was computed # words = 44784\n",
      "all accessor variety was computed # words = 44784\n",
      "n= 2 \n",
      "Coherence Score:  0.4668892156309924\n",
      "n= 3 \n",
      "Coherence Score:  0.4580620425150417\n",
      "n= 4 \n",
      "Coherence Score:  0.36433866554354855\n",
      "n= 5 \n",
      "Coherence Score:  0.43476294133711074\n",
      "n= 6 \n",
      "Coherence Score:  0.4090889166736602\n",
      "n= 7 \n",
      "Coherence Score:  0.40509714563241006\n",
      "n= 8 \n",
      "Coherence Score:  0.40480768498308706\n",
      "n= 9 \n",
      "Coherence Score:  0.4071223487605424\n",
      "training was done. used memory 1.015 Gbory 0.974 Gb\n",
      "all cohesion probabilities was computed. # words = 206808\n",
      "all branching entropies was computed # words = 134032\n",
      "all accessor variety was computed # words = 134032\n",
      "training was done. used memory 1.079 Gbory 1.042 Gb\n",
      "all cohesion probabilities was computed. # words = 36621\n",
      "all branching entropies was computed # words = 62711\n",
      "all accessor variety was computed # words = 62711\n",
      "n= 2 \n",
      "Coherence Score:  0.31846404125023847\n",
      "n= 3 \n",
      "Coherence Score:  0.376123625677705\n",
      "n= 4 \n",
      "Coherence Score:  0.4169627625733946\n",
      "n= 5 \n",
      "Coherence Score:  0.3915181411005685\n",
      "n= 6 \n",
      "Coherence Score:  0.40160635662847594\n",
      "n= 7 \n",
      "Coherence Score:  0.39490329993168033\n",
      "n= 8 \n",
      "Coherence Score:  0.4206656240174817\n",
      "n= 9 \n",
      "Coherence Score:  0.48102799369611327\n",
      "training was done. used memory 1.095 Gbory 1.089 Gb\n",
      "all cohesion probabilities was computed. # words = 218750\n",
      "all branching entropies was computed # words = 141592\n",
      "all accessor variety was computed # words = 141592\n",
      "training was done. used memory 1.123 Gbory 1.123 Gb\n",
      "all cohesion probabilities was computed. # words = 21912\n",
      "all branching entropies was computed # words = 35151\n",
      "all accessor variety was computed # words = 35151\n",
      "n= 2 \n",
      "Coherence Score:  0.3947409682042108\n",
      "n= 3 \n",
      "Coherence Score:  0.35705316645584756\n",
      "n= 4 \n",
      "Coherence Score:  0.4293680465645553\n",
      "n= 5 \n",
      "Coherence Score:  0.442519921694892\n",
      "n= 6 \n",
      "Coherence Score:  0.4865119757225584\n",
      "n= 7 \n",
      "Coherence Score:  0.4493649214700336\n",
      "n= 8 \n",
      "Coherence Score:  0.47450163170107984\n",
      "n= 9 \n",
      "Coherence Score:  0.5259653982620578\n",
      "training was done. used memory 0.980 Gbory 0.975 Gb\n",
      "all cohesion probabilities was computed. # words = 196447\n",
      "all branching entropies was computed # words = 126295\n",
      "all accessor variety was computed # words = 126295\n",
      "training was done. used memory 0.974 Gbory 0.959 Gb\n",
      "all cohesion probabilities was computed. # words = 28239\n",
      "all branching entropies was computed # words = 43457\n",
      "all accessor variety was computed # words = 43457\n",
      "n= 2 \n",
      "Coherence Score:  0.5330262719395091\n",
      "n= 3 \n",
      "Coherence Score:  0.568826689227039\n",
      "n= 4 \n",
      "Coherence Score:  0.5499603141532713\n",
      "n= 5 \n",
      "Coherence Score:  0.5547063639324898\n",
      "n= 6 \n",
      "Coherence Score:  0.5470397527019318\n",
      "n= 7 \n",
      "Coherence Score:  0.5538820885406119\n",
      "n= 8 \n",
      "Coherence Score:  0.5600061957425568\n",
      "n= 9 \n",
      "Coherence Score:  0.5410284092014938\n",
      "training was done. used memory 1.013 Gbory 1.007 Gb\n",
      "all cohesion probabilities was computed. # words = 198382\n",
      "all branching entropies was computed # words = 129216\n",
      "all accessor variety was computed # words = 129216\n",
      "training was done. used memory 1.076 Gbory 1.041 Gb\n",
      "all cohesion probabilities was computed. # words = 33490\n",
      "all branching entropies was computed # words = 55331\n",
      "all accessor variety was computed # words = 55331\n",
      "n= 2 \n",
      "Coherence Score:  0.24096266974439148\n",
      "n= 3 \n",
      "Coherence Score:  0.35445556584590276\n",
      "n= 4 \n",
      "Coherence Score:  0.30642720099323134\n",
      "n= 5 \n",
      "Coherence Score:  0.3138472338557029\n",
      "n= 6 \n",
      "Coherence Score:  0.3924953282451864\n",
      "n= 7 \n",
      "Coherence Score:  0.4032498961346113\n",
      "n= 8 \n",
      "Coherence Score:  0.42221021830011707\n",
      "n= 9 \n",
      "Coherence Score:  0.3985063149076184\n",
      "training was done. used memory 1.076 Gbory 1.066 Gb\n",
      "all cohesion probabilities was computed. # words = 228985\n",
      "all branching entropies was computed # words = 148357\n",
      "all accessor variety was computed # words = 148357\n",
      "training was done. used memory 1.110 Gbory 1.110 Gb\n",
      "all cohesion probabilities was computed. # words = 23388\n",
      "all branching entropies was computed # words = 42021\n",
      "all accessor variety was computed # words = 42021\n",
      "n= 2 \n",
      "Coherence Score:  0.5417238902178108\n",
      "n= 3 \n",
      "Coherence Score:  0.5551621930413332\n",
      "n= 4 \n",
      "Coherence Score:  0.5225147098880422\n",
      "n= 5 \n",
      "Coherence Score:  0.48116529553445686\n",
      "n= 6 \n",
      "Coherence Score:  0.5050833112986944\n",
      "n= 7 \n",
      "Coherence Score:  0.47407278762091104\n",
      "n= 8 \n",
      "Coherence Score:  0.487785154833326\n",
      "n= 9 \n",
      "Coherence Score:  0.5167207367566847\n",
      "training was done. used memory 1.082 Gbory 1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 153000\n",
      "all branching entropies was computed # words = 102348\n",
      "all accessor variety was computed # words = 102348\n",
      "training was done. used memory 1.066 Gbory 1.095 Gb\n",
      "all cohesion probabilities was computed. # words = 36228\n",
      "all branching entropies was computed # words = 63368\n",
      "all accessor variety was computed # words = 63368\n",
      "n= 2 \n",
      "Coherence Score:  0.45456545028641726\n",
      "n= 3 \n",
      "Coherence Score:  0.40378764616337026\n",
      "n= 4 \n",
      "Coherence Score:  0.4133747496939706\n",
      "n= 5 \n",
      "Coherence Score:  0.4208316329183324\n",
      "n= 6 \n",
      "Coherence Score:  0.428110424692233\n",
      "n= 7 \n",
      "Coherence Score:  0.4292938767062498\n",
      "n= 8 \n",
      "Coherence Score:  0.4323656835393847\n",
      "n= 9 \n",
      "Coherence Score:  0.4357453610095683\n",
      "training was done. used memory 1.129 Gbory 1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 202515\n",
      "all branching entropies was computed # words = 134427\n",
      "all accessor variety was computed # words = 134427\n",
      "training was done. used memory 1.148 Gbory 1.123 Gb\n",
      "all cohesion probabilities was computed. # words = 35100\n",
      "all branching entropies was computed # words = 58124\n",
      "all accessor variety was computed # words = 58124\n",
      "n= 2 \n",
      "Coherence Score:  0.4581258668067975\n",
      "n= 3 \n",
      "Coherence Score:  0.45802383425974913\n",
      "n= 4 \n",
      "Coherence Score:  0.47574080936059365\n",
      "n= 5 \n",
      "Coherence Score:  0.4634622915821943\n",
      "n= 6 \n",
      "Coherence Score:  0.42326625753402713\n",
      "n= 7 \n",
      "Coherence Score:  0.4146270048366198\n",
      "n= 8 \n",
      "Coherence Score:  0.4366513306992028\n",
      "n= 9 \n",
      "Coherence Score:  0.4219208721022678\n",
      "training was done. used memory 1.124 Gbory 1.127 Gb\n",
      "all cohesion probabilities was computed. # words = 197104\n",
      "all branching entropies was computed # words = 128314\n",
      "all accessor variety was computed # words = 128314\n",
      "training was done. used memory 1.151 Gbory 1.151 Gb\n",
      "all cohesion probabilities was computed. # words = 24254\n",
      "all branching entropies was computed # words = 40315\n",
      "all accessor variety was computed # words = 40315\n",
      "n= 2 \n",
      "Coherence Score:  0.3485704598280423\n",
      "n= 3 \n",
      "Coherence Score:  0.4229187437405862\n",
      "n= 4 \n",
      "Coherence Score:  0.39793110620969063\n",
      "n= 5 \n",
      "Coherence Score:  0.38115212623235095\n",
      "n= 6 \n",
      "Coherence Score:  0.4293386696768753\n",
      "n= 7 \n",
      "Coherence Score:  0.46753150506267893\n",
      "n= 8 \n",
      "Coherence Score:  0.42736920110920873\n",
      "n= 9 \n",
      "Coherence Score:  0.4579810455350359\n",
      "training was done. used memory 1.136 Gbory 1.132 Gb\n",
      "all cohesion probabilities was computed. # words = 161785\n",
      "all branching entropies was computed # words = 106150\n",
      "all accessor variety was computed # words = 106150\n",
      "training was done. used memory 1.143 Gbory 1.143 Gb\n",
      "all cohesion probabilities was computed. # words = 15898\n",
      "all branching entropies was computed # words = 29878\n",
      "all accessor variety was computed # words = 29878\n",
      "n= 2 \n",
      "Coherence Score:  0.3385756624426808\n",
      "n= 3 \n",
      "Coherence Score:  0.4491215167313409\n",
      "n= 4 \n",
      "Coherence Score:  0.3389322264448463\n",
      "n= 5 \n",
      "Coherence Score:  0.3486087375391637\n",
      "n= 6 \n",
      "Coherence Score:  0.38693092443231336\n",
      "n= 7 \n",
      "Coherence Score:  0.4086375684678078\n",
      "n= 8 \n",
      "Coherence Score:  0.41817841906130754\n",
      "n= 9 \n",
      "Coherence Score:  0.4104838230286806\n",
      "training was done. used memory 1.117 Gbory 1.092 Gb\n",
      "all cohesion probabilities was computed. # words = 103407\n",
      "all branching entropies was computed # words = 67590\n",
      "all accessor variety was computed # words = 67590\n",
      "training was done. used memory 1.102 Gbory 1.117 Gb\n",
      "all cohesion probabilities was computed. # words = 35635\n",
      "all branching entropies was computed # words = 59940\n",
      "all accessor variety was computed # words = 59940\n",
      "n= 2 \n",
      "Coherence Score:  0.4244321922323351\n",
      "n= 3 \n",
      "Coherence Score:  0.38685274848741075\n",
      "n= 4 \n",
      "Coherence Score:  0.36500320942563963\n",
      "n= 5 \n",
      "Coherence Score:  0.39871191039295234\n",
      "n= 6 \n",
      "Coherence Score:  0.40051833983430757\n",
      "n= 7 \n",
      "Coherence Score:  0.3787389157198396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 8 \n",
      "Coherence Score:  0.3757780426643603\n",
      "n= 9 \n",
      "Coherence Score:  0.40661758750080823\n",
      "training was done. used memory 1.121 Gbory 1.108 Gb\n",
      "all cohesion probabilities was computed. # words = 187207\n",
      "all branching entropies was computed # words = 123923\n",
      "all accessor variety was computed # words = 123923\n",
      "training was done. used memory 1.129 Gbory 1.129 Gb\n",
      "all cohesion probabilities was computed. # words = 23910\n",
      "all branching entropies was computed # words = 34865\n",
      "all accessor variety was computed # words = 34865\n",
      "n= 2 \n",
      "Coherence Score:  0.35995522437341376\n",
      "n= 3 \n",
      "Coherence Score:  0.35826789436246437\n",
      "n= 4 \n",
      "Coherence Score:  0.33570137207721024\n",
      "n= 5 \n",
      "Coherence Score:  0.38461253502994036\n",
      "n= 6 \n",
      "Coherence Score:  0.3296779338626427\n",
      "n= 7 \n",
      "Coherence Score:  0.37225537621721766\n",
      "n= 8 \n",
      "Coherence Score:  0.36475232515809974\n",
      "n= 9 \n",
      "Coherence Score:  0.384594320120199\n",
      "training was done. used memory 1.109 Gbory 1.117 Gb\n",
      "all cohesion probabilities was computed. # words = 175280\n",
      "all branching entropies was computed # words = 112426\n",
      "all accessor variety was computed # words = 112426\n",
      "training was done. used memory 1.129 Gbry 1.104 Gb\n",
      "all cohesion probabilities was computed. # words = 13829\n",
      "all branching entropies was computed # words = 30452\n",
      "all accessor variety was computed # words = 30452\n",
      "n= 2 \n",
      "Coherence Score:  0.3907968903870168\n",
      "n= 3 \n",
      "Coherence Score:  0.39340315528668907\n",
      "n= 4 \n",
      "Coherence Score:  0.42864294614938614\n",
      "n= 5 \n",
      "Coherence Score:  0.37185495310032923\n",
      "n= 6 \n",
      "Coherence Score:  0.40756652999999093\n",
      "n= 7 \n",
      "Coherence Score:  0.4662846054857191\n",
      "n= 8 \n",
      "Coherence Score:  0.43860412403137095\n",
      "n= 9 \n",
      "Coherence Score:  0.4317023675351439\n",
      "training was done. used memory 1.118 Gbry 1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 88191\n",
      "all branching entropies was computed # words = 63068\n",
      "all accessor variety was computed # words = 63068\n",
      "training was done. used memory 1.118 Gbry 1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 5028\n",
      "all branching entropies was computed # words = 12650\n",
      "all accessor variety was computed # words = 12650\n",
      "n= 2 \n",
      "Coherence Score:  0.4417231669789068\n",
      "n= 3 \n",
      "Coherence Score:  0.43040734749405757\n",
      "n= 4 \n",
      "Coherence Score:  0.42584321938412284\n",
      "n= 5 \n",
      "Coherence Score:  0.44881391703912604\n",
      "n= 6 \n",
      "Coherence Score:  0.4577662802727032\n",
      "n= 7 \n",
      "Coherence Score:  0.46012654590730523\n",
      "n= 8 \n",
      "Coherence Score:  0.45214266896259603\n",
      "n= 9 \n",
      "Coherence Score:  0.4651321179975941\n",
      "training was done. used memory 1.096 Gbry 1.096 Gb\n",
      "all cohesion probabilities was computed. # words = 33539\n",
      "all branching entropies was computed # words = 23449\n",
      "all accessor variety was computed # words = 23449\n",
      "training was done. used memory 1.059 Gbory 1.096 Gb\n",
      "all cohesion probabilities was computed. # words = 21860\n",
      "all branching entropies was computed # words = 37535\n",
      "all accessor variety was computed # words = 37535\n",
      "n= 2 \n",
      "Coherence Score:  0.42118174459199276\n",
      "n= 3 \n",
      "Coherence Score:  0.41711138199609477\n",
      "n= 4 \n",
      "Coherence Score:  0.3751679467990353\n",
      "n= 5 \n",
      "Coherence Score:  0.41999935595895244\n",
      "n= 6 \n",
      "Coherence Score:  0.38040841151796395\n",
      "n= 7 \n",
      "Coherence Score:  0.4066194826597584\n",
      "n= 8 \n",
      "Coherence Score:  0.4341579830053589\n",
      "n= 9 \n",
      "Coherence Score:  0.4193823814167964\n",
      "training was done. used memory 1.085 Gbory 1.060 Gb\n",
      "all cohesion probabilities was computed. # words = 144086\n",
      "all branching entropies was computed # words = 96219\n",
      "all accessor variety was computed # words = 96219\n",
      "training was done. used memory 1.056 Gbory 1.087 Gb\n",
      "all cohesion probabilities was computed. # words = 23211\n",
      "all branching entropies was computed # words = 41123\n",
      "all accessor variety was computed # words = 41123\n",
      "n= 2 \n",
      "Coherence Score:  0.3467242016465811\n",
      "n= 3 \n",
      "Coherence Score:  0.3835346671365463\n",
      "n= 4 \n",
      "Coherence Score:  0.41544578704374324\n",
      "n= 5 \n",
      "Coherence Score:  0.41886851512393497\n",
      "n= 6 \n",
      "Coherence Score:  0.43784789741743463\n",
      "n= 7 \n",
      "Coherence Score:  0.45837073591923466\n",
      "n= 8 \n",
      "Coherence Score:  0.4384680010125528\n",
      "n= 9 \n",
      "Coherence Score:  0.41701002517189983\n",
      "training was done. used memory 1.083 Gbory 1.059 Gb\n",
      "all cohesion probabilities was computed. # words = 136794\n",
      "all branching entropies was computed # words = 89721\n",
      "all accessor variety was computed # words = 89721\n",
      "training was done. used memory 1.083 Gbory 1.083 Gb\n",
      "all cohesion probabilities was computed. # words = 16664\n",
      "all branching entropies was computed # words = 28463\n",
      "all accessor variety was computed # words = 28463\n",
      "n= 2 \n",
      "Coherence Score:  0.3390198229737884\n",
      "n= 3 \n",
      "Coherence Score:  0.327498265223706\n",
      "n= 4 \n",
      "Coherence Score:  0.36564004684164453\n",
      "n= 5 \n",
      "Coherence Score:  0.3932779807913703\n",
      "n= 6 \n",
      "Coherence Score:  0.3829059279304201\n",
      "n= 7 \n",
      "Coherence Score:  0.36882073193271997\n",
      "n= 8 \n",
      "Coherence Score:  0.4113495213173034\n",
      "n= 9 \n",
      "Coherence Score:  0.4092404703604956\n",
      "training was done. used memory 1.076 Gbory 1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 112557\n",
      "all branching entropies was computed # words = 71968\n",
      "all accessor variety was computed # words = 71968\n",
      "training was done. used memory 1.060 Gbory 1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 38054\n",
      "all branching entropies was computed # words = 60419\n",
      "all accessor variety was computed # words = 60419\n",
      "n= 2 \n",
      "Coherence Score:  0.45401400449382046\n",
      "n= 3 \n",
      "Coherence Score:  0.4441441082992282\n",
      "n= 4 \n",
      "Coherence Score:  0.4161171306677492\n",
      "n= 5 \n",
      "Coherence Score:  0.385132397748448\n",
      "n= 6 \n",
      "Coherence Score:  0.39309549688027295\n",
      "n= 7 \n",
      "Coherence Score:  0.3906452057926422\n",
      "n= 8 \n",
      "Coherence Score:  0.42074265675616795\n",
      "n= 9 \n",
      "Coherence Score:  0.4476231686874963\n",
      "training was done. used memory 1.201 Gbory 1.134 Gb\n",
      "all cohesion probabilities was computed. # words = 239945\n",
      "all branching entropies was computed # words = 159258\n",
      "all accessor variety was computed # words = 159258\n",
      "training was done. used memory 1.230 Gb\n",
      "all cohesion probabilities was computed. # words = 2578\n",
      "all branching entropies was computed # words = 5354\n",
      "all accessor variety was computed # words = 5354\n",
      "n= 2 \n",
      "Coherence Score:  0.3797989337487029\n",
      "n= 3 \n",
      "Coherence Score:  0.3819972839785402\n",
      "n= 4 \n",
      "Coherence Score:  0.39468309643135246\n",
      "n= 5 \n",
      "Coherence Score:  0.4495308527740359\n",
      "n= 6 \n",
      "Coherence Score:  0.4518316720470152\n",
      "n= 7 \n",
      "Coherence Score:  0.4227584072809242\n",
      "n= 8 \n",
      "Coherence Score:  0.4135394405070841\n",
      "n= 9 \n",
      "Coherence Score:  0.4199996677294453\n",
      "training was done. used memory 1.065 Gb\n",
      "all cohesion probabilities was computed. # words = 25657\n",
      "all branching entropies was computed # words = 16172\n",
      "all accessor variety was computed # words = 16172\n",
      "training was done. used memory 1.026 Gbory 1.060 Gb\n",
      "all cohesion probabilities was computed. # words = 36041\n",
      "all branching entropies was computed # words = 59651\n",
      "all accessor variety was computed # words = 59651\n",
      "n= 2 \n",
      "Coherence Score:  0.4657044173509144\n",
      "n= 3 \n",
      "Coherence Score:  0.48073384428070076\n",
      "n= 4 \n",
      "Coherence Score:  0.44569992572514133\n",
      "n= 5 \n",
      "Coherence Score:  0.3990443898784267\n",
      "n= 6 \n",
      "Coherence Score:  0.48049936992758857\n",
      "n= 7 \n",
      "Coherence Score:  0.43148902182135834\n",
      "n= 8 \n",
      "Coherence Score:  0.4696353751743169\n",
      "n= 9 \n",
      "Coherence Score:  0.4552507262362304\n",
      "training was done. used memory 1.044 Gbory 1.039 Gb\n",
      "all cohesion probabilities was computed. # words = 185562\n",
      "all branching entropies was computed # words = 121622\n",
      "all accessor variety was computed # words = 121622\n",
      "training was done. used memory 1.089 Gbory 1.081 Gb\n",
      "all cohesion probabilities was computed. # words = 32255\n",
      "all branching entropies was computed # words = 52566\n",
      "all accessor variety was computed # words = 52566\n",
      "n= 2 \n",
      "Coherence Score:  0.3266451066929589\n",
      "n= 3 \n",
      "Coherence Score:  0.436073019041123\n",
      "n= 4 \n",
      "Coherence Score:  0.4329728516540423\n",
      "n= 5 \n",
      "Coherence Score:  0.4593166593812822\n",
      "n= 6 \n",
      "Coherence Score:  0.4240794571274127\n",
      "n= 7 \n",
      "Coherence Score:  0.4551135459503097\n",
      "n= 8 \n",
      "Coherence Score:  0.4575720465262504\n",
      "n= 9 \n",
      "Coherence Score:  0.3888059336781439\n",
      "training was done. used memory 1.084 Gbory 1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 193174\n",
      "all branching entropies was computed # words = 125958\n",
      "all accessor variety was computed # words = 125958\n",
      "training was done. used memory 1.167 Gbory 1.096 Gb\n",
      "all cohesion probabilities was computed. # words = 49233\n",
      "all branching entropies was computed # words = 79322\n",
      "all accessor variety was computed # words = 79322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 2 \n",
      "Coherence Score:  0.3651445811823606\n",
      "n= 3 \n",
      "Coherence Score:  0.30021456514250827\n",
      "n= 4 \n",
      "Coherence Score:  0.34355067047970433\n",
      "n= 5 \n",
      "Coherence Score:  0.3903826674237691\n",
      "n= 6 \n",
      "Coherence Score:  0.4019806544235698\n",
      "n= 7 \n",
      "Coherence Score:  0.4350707797578293\n",
      "n= 8 \n",
      "Coherence Score:  0.41942529055495736\n",
      "n= 9 \n",
      "Coherence Score:  0.35288167198788156\n",
      "training was done. used memory 1.286 Gbory 1.290 Gb\n",
      "all cohesion probabilities was computed. # words = 282885\n",
      "all branching entropies was computed # words = 180200\n",
      "all accessor variety was computed # words = 180200\n",
      "training was done. used memory 1.321 Gbory 1.340 Gb\n",
      "all cohesion probabilities was computed. # words = 37320\n",
      "all branching entropies was computed # words = 63123\n",
      "all accessor variety was computed # words = 63123\n",
      "n= 2 \n",
      "Coherence Score:  0.3664403352522194\n",
      "n= 3 \n",
      "Coherence Score:  0.3537279055327664\n",
      "n= 4 \n",
      "Coherence Score:  0.4138831364519793\n",
      "n= 5 \n",
      "Coherence Score:  0.33006537050516216\n",
      "n= 6 \n",
      "Coherence Score:  0.3598504489708389\n",
      "n= 7 \n",
      "Coherence Score:  0.39170496030137336\n",
      "n= 8 \n",
      "Coherence Score:  0.33081884930234084\n",
      "n= 9 \n",
      "Coherence Score:  0.3690059012452763\n",
      "training was done. used memory 1.217 Gbory 1.217 Gb\n",
      "all cohesion probabilities was computed. # words = 208626\n",
      "all branching entropies was computed # words = 135273\n",
      "all accessor variety was computed # words = 135273\n",
      "training was done. used memory 1.256 Gb\n",
      "all cohesion probabilities was computed. # words = 28575\n",
      "all branching entropies was computed # words = 49713\n",
      "all accessor variety was computed # words = 49713\n",
      "n= 2 \n",
      "Coherence Score:  0.350656770116868\n",
      "n= 3 \n",
      "Coherence Score:  0.4043335354423394\n",
      "n= 4 \n",
      "Coherence Score:  0.43590674333032\n",
      "n= 5 \n",
      "Coherence Score:  0.43320496465893427\n",
      "n= 6 \n",
      "Coherence Score:  0.46141253644800534\n",
      "n= 7 \n",
      "Coherence Score:  0.436868834741927\n",
      "n= 8 \n",
      "Coherence Score:  0.4891208508822375\n",
      "n= 9 \n",
      "Coherence Score:  0.4679889490051561\n",
      "training was done. used memory 1.191 Gb\n",
      "all cohesion probabilities was computed. # words = 175524\n",
      "all branching entropies was computed # words = 117810\n",
      "all accessor variety was computed # words = 117810\n",
      "training was done. used memory 1.218 Gby 1.218 Gb\n",
      "all cohesion probabilities was computed. # words = 894\n",
      "all branching entropies was computed # words = 3203\n",
      "all accessor variety was computed # words = 3203\n",
      "n= 2 \n",
      "Coherence Score:  0.4503905997236616\n",
      "n= 3 \n",
      "Coherence Score:  0.4811102947263584\n",
      "n= 4 \n",
      "Coherence Score:  0.43128588534258383\n",
      "n= 5 \n",
      "Coherence Score:  0.48684889900155187\n",
      "n= 6 \n",
      "Coherence Score:  0.4987735305338734\n",
      "n= 7 \n",
      "Coherence Score:  0.48479223691839973\n",
      "n= 8 \n",
      "Coherence Score:  0.48186148494485126\n",
      "n= 9 \n",
      "Coherence Score:  0.5080715822833424\n",
      "training was done. used memory 1.088 Gby 1.088 Gb\n",
      "all cohesion probabilities was computed. # words = 11824\n",
      "all branching entropies was computed # words = 7741\n",
      "all accessor variety was computed # words = 7741\n",
      "training was done. used memory 1.088 Gbory 1.088 Gb\n",
      "all cohesion probabilities was computed. # words = 33066\n",
      "all branching entropies was computed # words = 51193\n",
      "all accessor variety was computed # words = 51193\n",
      "n= 2 \n",
      "Coherence Score:  0.5029097557995201\n",
      "n= 3 \n",
      "Coherence Score:  0.4686698726750209\n",
      "n= 4 \n",
      "Coherence Score:  0.474386459920116\n",
      "n= 5 \n",
      "Coherence Score:  0.49494387358356684\n",
      "n= 6 \n",
      "Coherence Score:  0.44949435590610426\n",
      "n= 7 \n",
      "Coherence Score:  0.4916386409913149\n",
      "n= 8 \n",
      "Coherence Score:  0.4388250133882998\n",
      "n= 9 \n",
      "Coherence Score:  0.4136587240776227\n",
      "training was done. used memory 1.104 Gbory 1.093 Gb\n",
      "all cohesion probabilities was computed. # words = 210601\n",
      "all branching entropies was computed # words = 138033\n",
      "all accessor variety was computed # words = 138033\n",
      "training was done. used memory 1.146 Gbory 1.175 Gb\n",
      "all cohesion probabilities was computed. # words = 31339\n",
      "all branching entropies was computed # words = 48209\n",
      "all accessor variety was computed # words = 48209\n",
      "n= 2 \n",
      "Coherence Score:  0.38081798421152324\n",
      "n= 3 \n",
      "Coherence Score:  0.4601619315717767\n",
      "n= 4 \n",
      "Coherence Score:  0.402858354083116\n",
      "n= 5 \n",
      "Coherence Score:  0.4447500352070833\n",
      "n= 6 \n",
      "Coherence Score:  0.44193905073814843\n",
      "n= 7 \n",
      "Coherence Score:  0.4655065481777867\n",
      "n= 8 \n",
      "Coherence Score:  0.48061721576484956\n",
      "n= 9 \n",
      "Coherence Score:  0.4383050460992334\n",
      "training was done. used memory 1.128 Gbory 1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 215246\n",
      "all branching entropies was computed # words = 138363\n",
      "all accessor variety was computed # words = 138363\n",
      "training was done. used memory 1.128 Gbory 1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 40463\n",
      "all branching entropies was computed # words = 81225\n",
      "all accessor variety was computed # words = 81225\n",
      "n= 2 \n",
      "Coherence Score:  0.5469053415064298\n",
      "n= 3 \n",
      "Coherence Score:  0.49902161203718576\n",
      "n= 4 \n",
      "Coherence Score:  0.48974845004716294\n",
      "n= 5 \n",
      "Coherence Score:  0.47370238297061584\n",
      "n= 6 \n",
      "Coherence Score:  0.4752860726807144\n",
      "n= 7 \n",
      "Coherence Score:  0.4260636847927589\n",
      "n= 8 \n",
      "Coherence Score:  0.4498135444387701\n",
      "n= 9 \n",
      "Coherence Score:  0.4150987895391476\n",
      "training was done. used memory 1.135 Gbory 1.132 Gb\n",
      "all cohesion probabilities was computed. # words = 216375\n",
      "all branching entropies was computed # words = 151160\n",
      "all accessor variety was computed # words = 151160\n",
      "training was done. used memory 1.167 Gbory 1.167 Gb\n",
      "all cohesion probabilities was computed. # words = 29340\n",
      "all branching entropies was computed # words = 55073\n",
      "all accessor variety was computed # words = 55073\n",
      "n= 2 \n",
      "Coherence Score:  0.5365427026874746\n",
      "n= 3 \n",
      "Coherence Score:  0.4501119514255712\n",
      "n= 4 \n",
      "Coherence Score:  0.4636875709694677\n",
      "n= 5 \n",
      "Coherence Score:  0.48254411614027626\n",
      "n= 6 \n",
      "Coherence Score:  0.5206135789919403\n",
      "n= 7 \n",
      "Coherence Score:  0.49779851216751175\n",
      "n= 8 \n",
      "Coherence Score:  0.4658584866111734\n",
      "n= 9 \n",
      "Coherence Score:  0.4786906459672228\n",
      "training was done. used memory 1.140 Gbory 1.150 Gb\n",
      "all cohesion probabilities was computed. # words = 187295\n",
      "all branching entropies was computed # words = 126993\n",
      "all accessor variety was computed # words = 126993\n",
      "training was done. used memory 1.162 Gb\n",
      "all cohesion probabilities was computed. # words = 61649\n",
      "all branching entropies was computed # words = 118138\n",
      "all accessor variety was computed # words = 118138\n",
      "n= 2 \n",
      "Coherence Score:  0.5436141272307287\n",
      "n= 3 \n",
      "Coherence Score:  0.48787581062126356\n",
      "n= 4 \n",
      "Coherence Score:  0.5020681343943842\n",
      "n= 5 \n",
      "Coherence Score:  0.49656486874782846\n",
      "n= 6 \n",
      "Coherence Score:  0.4754365163792295\n",
      "n= 7 \n",
      "Coherence Score:  0.4871940544004065\n",
      "n= 8 \n",
      "Coherence Score:  0.4420909038492893\n",
      "n= 9 \n",
      "Coherence Score:  0.4427366016627263\n",
      "training was done. used memory 1.432 Gb\n",
      "all cohesion probabilities was computed. # words = 271905\n",
      "all branching entropies was computed # words = 190752\n",
      "all accessor variety was computed # words = 190752\n",
      "training was done. used memory 1.421 Gbory 1.421 Gb\n",
      "all cohesion probabilities was computed. # words = 38432\n",
      "all branching entropies was computed # words = 75063\n",
      "all accessor variety was computed # words = 75063\n",
      "n= 2 \n",
      "Coherence Score:  0.47311212140719994\n",
      "n= 3 \n",
      "Coherence Score:  0.37415178924938003\n",
      "n= 4 \n",
      "Coherence Score:  0.4263284486140127\n",
      "n= 5 \n",
      "Coherence Score:  0.4202691124939574\n",
      "n= 6 \n",
      "Coherence Score:  0.47525542864225595\n",
      "n= 7 \n",
      "Coherence Score:  0.4844780407974531\n",
      "n= 8 \n",
      "Coherence Score:  0.41282562051922717\n",
      "n= 9 \n",
      "Coherence Score:  0.4312367255340821\n",
      "training was done. used memory 1.230 Gbory 1.245 Gb\n",
      "all cohesion probabilities was computed. # words = 196195\n",
      "all branching entropies was computed # words = 131474\n",
      "all accessor variety was computed # words = 131474\n",
      "training was done. used memory 1.229 Gbory 1.229 Gb\n",
      "all cohesion probabilities was computed. # words = 41748\n",
      "all branching entropies was computed # words = 71444\n",
      "all accessor variety was computed # words = 71444\n",
      "n= 2 \n",
      "Coherence Score:  0.4302463786492093\n",
      "n= 3 \n",
      "Coherence Score:  0.4055176418925191\n",
      "n= 4 \n",
      "Coherence Score:  0.44868820597634\n",
      "n= 5 \n",
      "Coherence Score:  0.43895866625452695\n",
      "n= 6 \n",
      "Coherence Score:  0.4732185217177845\n",
      "n= 7 \n",
      "Coherence Score:  0.4296240845813493\n",
      "n= 8 \n",
      "Coherence Score:  0.47898409081457993\n",
      "n= 9 \n",
      "Coherence Score:  0.4809748907750553\n",
      "training was done. used memory 1.285 Gbory 1.248 Gb\n",
      "all cohesion probabilities was computed. # words = 253896\n",
      "all branching entropies was computed # words = 165902\n",
      "all accessor variety was computed # words = 165902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.275 Gbory 1.307 Gb\n",
      "all cohesion probabilities was computed. # words = 22040\n",
      "all branching entropies was computed # words = 37482\n",
      "all accessor variety was computed # words = 37482\n",
      "n= 2 \n",
      "Coherence Score:  0.4083573276962529\n",
      "n= 3 \n",
      "Coherence Score:  0.34067655745959785\n",
      "n= 4 \n",
      "Coherence Score:  0.3882033393292096\n",
      "n= 5 \n",
      "Coherence Score:  0.42912025156628075\n",
      "n= 6 \n",
      "Coherence Score:  0.409349014218263\n",
      "n= 7 \n",
      "Coherence Score:  0.4064136812192884\n",
      "n= 8 \n",
      "Coherence Score:  0.3777773802038065\n",
      "n= 9 \n",
      "Coherence Score:  0.4053625703413465\n",
      "training was done. used memory 1.251 Gbory 1.251 Gb\n",
      "all cohesion probabilities was computed. # words = 143961\n",
      "all branching entropies was computed # words = 95721\n",
      "all accessor variety was computed # words = 95721\n",
      "training was done. used memory 1.250 Gbory 1.250 Gb\n",
      "all cohesion probabilities was computed. # words = 16834\n",
      "all branching entropies was computed # words = 28132\n",
      "all accessor variety was computed # words = 28132\n",
      "n= 2 \n",
      "Coherence Score:  0.2783414523336276\n",
      "n= 3 \n",
      "Coherence Score:  0.3716731930634108\n",
      "n= 4 \n",
      "Coherence Score:  0.37425955707467573\n",
      "n= 5 \n",
      "Coherence Score:  0.34755378262265124\n",
      "n= 6 \n",
      "Coherence Score:  0.3734054255989084\n",
      "n= 7 \n",
      "Coherence Score:  0.3842504588013148\n",
      "n= 8 \n",
      "Coherence Score:  0.4178841058997339\n",
      "n= 9 \n",
      "Coherence Score:  0.4464751274138093\n",
      "training was done. used memory 1.212 Gbory 1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 122772\n",
      "all branching entropies was computed # words = 80868\n",
      "all accessor variety was computed # words = 80868\n",
      "training was done. used memory 1.212 Gbory 1.212 Gb\n",
      "all cohesion probabilities was computed. # words = 20152\n",
      "all branching entropies was computed # words = 34080\n",
      "all accessor variety was computed # words = 34080\n",
      "n= 2 \n",
      "Coherence Score:  0.5041134758825992\n",
      "n= 3 \n",
      "Coherence Score:  0.4721066235174924\n",
      "n= 4 \n",
      "Coherence Score:  0.4900728905235967\n",
      "n= 5 \n",
      "Coherence Score:  0.47967773789966567\n",
      "n= 6 \n",
      "Coherence Score:  0.49682583167360544\n",
      "n= 7 \n",
      "Coherence Score:  0.42703042666552155\n",
      "n= 8 \n",
      "Coherence Score:  0.456018342492002\n",
      "n= 9 \n",
      "Coherence Score:  0.42121425350665676\n",
      "training was done. used memory 1.213 Gbory 1.209 Gb\n",
      "all cohesion probabilities was computed. # words = 145062\n",
      "all branching entropies was computed # words = 96348\n",
      "all accessor variety was computed # words = 96348\n"
     ]
    }
   ],
   "source": [
    "# keyword 전체 : 35개\n",
    "for keyword, subkeyword, interested_word in zip(keywords, subkeywords, interested_words):\n",
    "\n",
    "    # ## 테스트 용 : 1개\n",
    "    #keyword = keywords[0]\n",
    "    #subkeyword = subkeywords[0]\n",
    "    #interested_word = interested_words[0]\n",
    "\n",
    "    # 관심어(=명사) 추가\n",
    "    subkeyword = subkeyword.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "    interested_word = subkeyword + \"|\" + interested_word.replace(\" \", \"\").replace(\",\",\"|\")\n",
    "\n",
    "    new_nouns = []\n",
    "    new_nouns = new_nouns + keyword.split(' ')\n",
    "    new_nouns.append((keyword.replace(\" \", \"\")))\n",
    "    new_nouns = new_nouns + interested_word.split(\"|\")\n",
    "    new_nouns = list(set(new_nouns))\n",
    "\n",
    "    # new_nouns\n",
    "\n",
    "    for nouns in new_nouns:\n",
    "        twitter.add_dictionary(nouns, 'Noun')\n",
    "\n",
    "    df = hp.readall(keyword.replace(\" \",\"\"))\n",
    "    df = df[ (df['date'] >= '2019-07-01') & (df['date'] < '2020-07-01')]\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # df['text'] = df['title'].apply(hp.preprocessing) + df['content'].apply(hp.preprocessing)\n",
    "    df['text'] = df['title'] + df['content']\n",
    "    rows_date = df['date']\n",
    "\n",
    "    sentences = []\n",
    "    import re\n",
    "    for posts, dates in zip(df['text'], df['date']):\n",
    "        for post in re.split('\\?|\\.|!', posts):\n",
    "            post = hp.preprocessing(post)\n",
    "            sentences.append(post)\n",
    "\n",
    "    # 공백라인 및 NaN 제거\n",
    "    while(\"\" in sentences) : \n",
    "        sentences.remove(\"\") \n",
    "    while(\"NaN\" in sentences) : \n",
    "        sentences.remove(\"\") \n",
    "\n",
    "    # sentences\n",
    "\n",
    "    # ###########################################################################################\n",
    "    from soynlp.word import WordExtractor\n",
    "    word_extractor = WordExtractor()\n",
    "    word_extractor.train(sentences)\n",
    "    word_scores = word_extractor.extract()\n",
    "\n",
    "    from soynlp.tokenizer import LTokenizer\n",
    "    cohesion_scores = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "    ltokenizer = LTokenizer(scores = cohesion_scores)\n",
    "\n",
    "    tokened_sentences = []\n",
    "    for sentence in sentences:\n",
    "       tokened_sentences.append (ltokenizer.tokenize(sentence))\n",
    "\n",
    "    # tokened_sentences\n",
    "\n",
    "    ts = []\n",
    "    for s in tokened_sentences:\n",
    "        ts.append(hp.remove_stopwords(s))\n",
    "    #     print(s)\n",
    "\n",
    "\n",
    "    # # 공백라인 및 NaN 제거\n",
    "    while(\"\" in s) : \n",
    "        ts.remove(\"\") \n",
    "    while(\"NaN\" in sentences) : \n",
    "        ts.remove(\"\") \n",
    "\n",
    "    # ts\n",
    "\n",
    "    tsentences = []\n",
    "    for s in ts:\n",
    "        line = \"\"\n",
    "        for w in s:\n",
    "            line = line + \" \" + w\n",
    "        tsentences.append(line)\n",
    "\n",
    "    # 공백라인 및 NaN 제거\n",
    "    while(\"\" in s) : \n",
    "        tsentences.remove(\"\") \n",
    "    while(\"NaN\" in tsentences) : \n",
    "        tsentences.remove(\"\") \n",
    "\n",
    "    # 4. 각 문장별로 형태소 구분하기\n",
    "    sentences_tag = []\n",
    "    for sentence in tsentences:\n",
    "        morph = okt.pos(sentence)\n",
    "        sentences_tag.append(morph)\n",
    "        \n",
    "    # 5. 명사 혹은 형용사인 품사만 선별해 리스트에 담기\n",
    "    posts = []\n",
    "    for sentence1 in sentences_tag:\n",
    "        words = \"\"\n",
    "        for word, tag in sentence1:\n",
    "            if tag in ['Noun']:     # 명사만 추출, # 명사/형용사 추출은 ['Noun','Adjective']\n",
    "                words = words + word + \" \"\n",
    "        posts.append(words)\n",
    "\n",
    "    # print(len(posts), type(posts))\n",
    "    # print(len(rows_date), type(rows))\n",
    "\n",
    "    #7 공백라인 및 NaN 제거\n",
    "    while(\"\" in posts) : \n",
    "        posts.remove(\"\") \n",
    "    while(\"NaN\" in posts) : \n",
    "        posts.remove(\"\") \n",
    "    # posts\n",
    "\n",
    "    ## 최빈어\n",
    "\n",
    "    tf_list = []\n",
    "    for sentence1 in posts:\n",
    "        sentence1.split()\n",
    "        tf_list.append(sentence1.split())\n",
    "\n",
    "    lst = oneDArray(tf_list)\n",
    "\n",
    "    # 6. 선별된 품사별 빈도수 계산 & 상위 빈도 10위 까지 출력\n",
    "    counts = Counter(lst)\n",
    "\n",
    "    toplist = counts.most_common(500)\n",
    "    # type(toplist)\n",
    "\n",
    "    tf_df = pd.DataFrame (toplist,columns=['단어', '빈도수'])\n",
    "    # tf_df\n",
    "    #save_to_csv(keyword, \"최빈어\", tf_df)\n",
    "    save_to_csv(\"TM\", keyword, \"최빈어\", tf_df)\n",
    "\n",
    "    ## 토픽 모델링\n",
    "\n",
    "    dataset = pd.DataFrame(posts, columns=['문장'])\n",
    "\n",
    "    # nan_value = float(\"NaN\")\n",
    "    # dataset.replace(\"\", nan_value, inplace=True)\n",
    "    # dataset.dropna(subset = [\"문장\"], inplace=True)\n",
    "    # dataset.reindex\n",
    "\n",
    "    tmp_corpus = dataset['문장'].map(lambda x: x.split('.'))\n",
    "\n",
    "    #tmp_corpus\n",
    "\n",
    "    # type(tmp_corpus)\n",
    "    # tmp_corpus[11]\n",
    "    # tmp_corpus[13]\n",
    "\n",
    "    # corpus [[w1,w2,w3..],[..]]\n",
    "    corpus = []\n",
    "    for i in range(len(tmp_corpus)):\n",
    "        for line in tmp_corpus[i]:\n",
    "            words = [x for x in line.split()]\n",
    "            corpus.append(words)\n",
    "    #   corpus\n",
    "\n",
    "    num_of_sentences = len(corpus)\n",
    "    num_of_words = 0\n",
    "    for line in corpus:\n",
    "        num_of_words += len(line)\n",
    "\n",
    "    #print('Num of sentences - %s'%(num_of_sentences))\n",
    "    #print('Num of words - %s'%(num_of_words))\n",
    "\n",
    "    ### Gensim\n",
    "\n",
    "    nouns = corpus\n",
    "    bigram = gensim.models.Phrases(nouns)\n",
    "    trigram = gensim.models.Phrases(bigram[nouns])\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    bigram_document = [bigram_model[nouns] for nouns in nouns]\n",
    "    # bigram_document[1]\n",
    "    # bigram_document[3]\n",
    "\n",
    "    id2word = corpora.Dictionary(bigram_document)\n",
    "    corpus = [id2word.doc2bow(doc) for doc in bigram_document]\n",
    "    # corpus[0]\n",
    "\n",
    "    # corpus[7]  # 단어별 출현 출현 수\n",
    "\n",
    "    ### Topic Coherence 계산\n",
    "\n",
    "    # 토픽 모델링을 수행함에 있어서 적절한 토픽의 갯수를 찾는 것이 중요하다. \n",
    "    # 2부터 9까지 값을 늘려가면서 LDA 모델을 생성하여 각 모델의 coherence를 계산한다.\n",
    "    # 그리고, 적절한 토픽의 수는 토픽 갯수를 늘려가며 높은 coherence score 를 가지는 값으로 결정한다\n",
    "\n",
    "    coherence_score=[]\n",
    "    for i in range(2,10):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=i)\n",
    "        coherence_model = CoherenceModel(model, texts=bigram_document, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model.get_coherence()\n",
    "        print('n=',i,'\\nCoherence Score: ', coherence_lda)\n",
    "        coherence_score.append(coherence_lda)\n",
    "\n",
    "#     k=[]\n",
    "#     for i in range(2,10):\n",
    "#         k.append(i)\n",
    "\n",
    "#     x=numpy.array(k)\n",
    "#     y=numpy.array(coherence_score)\n",
    "#     title = f'{keyword} Topic Coherence'\n",
    "#     plt.title(title)\n",
    "#     plt.plot(x,y)\n",
    "#     plt.xlim(2,10)\n",
    "#     plt.xlabel('Number Of Topic (2-10)')\n",
    "#     plt.ylabel('Cohrence Score')\n",
    "#     plt.show()\n",
    "\n",
    "    model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=3)\n",
    "    # model.print_topics()\n",
    "    model.print_topics(num_words=500) # num_topics=20\n",
    "\n",
    "    _topics = getTopics(model)\n",
    "    #_topics\n",
    "\n",
    "    #type(_topics)\n",
    "\n",
    "    co_top_df = pd.DataFrame(_topics)\n",
    "\n",
    "    co_top_df.set_index(0)\n",
    "\n",
    "    save_to_csv(\"TM\", keyword, \"토픽\", co_top_df)\n",
    "    \n",
    "    ###################################################################################\n",
    "\n",
    "    from soynlp.word import WordExtractor\n",
    "\n",
    "    word_extractor = WordExtractor(min_frequency=1,\n",
    "        min_cohesion_forward=0.05, \n",
    "        min_right_branching_entropy=0.0\n",
    "    )\n",
    "    word_extractor.train(tsentences) # list of str or like\n",
    "    words = word_extractor.extract()\n",
    "\n",
    "    try:\n",
    "        words[keyword.replace(\" \", \"\")]\n",
    "        \n",
    "        def word_score(score):\n",
    "            return (score.cohesion_forward * math.exp(score.right_branching_entropy))\n",
    "\n",
    "        # print('단어   (빈도수, cohesion, branching entropy)\\n')\n",
    "        nouns = []\n",
    "        freqs = []\n",
    "        cohesions = []\n",
    "        entropys = []\n",
    "        for word, score in sorted(words.items(), key=lambda x:word_score(x[1]), reverse=True)[:500]:\n",
    "            nouns.append(word)\n",
    "            freqs.append(score.leftside_frequency)\n",
    "            cohesions.append(score.cohesion_forward)\n",
    "            entropys.append(score.right_branching_entropy)\n",
    "\n",
    "        words_df = pd.DataFrame({\"단어\": nouns, \"연관성\": cohesions, \"빈도수\": freqs, \"엔트로피\": entropys})\n",
    "        words_df = words_df.sort_values(['빈도수','연관성'])\n",
    "\n",
    "        save_to_csv(\"ckonlpy\", keyword, \"종합\", words_df)\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
